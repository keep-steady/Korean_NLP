{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. normalize_10days_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:27:55.445532Z",
     "start_time": "2018-03-07T13:27:55.440297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num file = 10\n",
      "/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10-24_article_all.txt\n",
      "/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10-29_article_all.txt\n",
      "/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10-21_article_all.txt\n",
      "/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10-27_article_all.txt\n",
      "/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10-23_article_all.txt\n",
      "/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10-28_article_all.txt\n",
      "/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10-26_article_all.txt\n",
      "/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10-20_article_all.txt\n",
      "/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10-25_article_all.txt\n",
      "/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10-22_article_all.txt\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "fnames = glob.glob('/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10*_article_all.txt')\n",
    "print('num file = {0}'.format(len(fnames)))\n",
    "for fname in fnames:\n",
    "    print(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각각의 텍스트파일을 열어서 파일의 이름 끝부분을 바꿈\n",
    "- 2016-10-24_article_all.txt -> 2016-10-24_article_all_norm.txt\n",
    "\n",
    "데이터는 한 줄이 하나의 문서로 나뉘어져있음. 각 문서에 문장의 구문은 double whitespace로 구분되어 있음. \n",
    "\n",
    "하나의 문서에서 문장을 가져온 뒤 normalize한 결과를 다시 double whitespace로 구분하느 코드를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:27:56.785816Z",
     "start_time": "2018-03-07T13:27:56.771599Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom soynlp.hangle import normalize\\n\\nfor fname in fnames:\\n    with open(fname, encoding='utf-8') as f:\\n        docs = [doc.strip() for doc in f]\\n        \\n    normed_docs = []\\n    for doc in docs:\\n        sents = doc.split('  ')\\n        sents = [normalize(sent, number=True) for sent in sents]\\n        sents = [sent for sent in sents if sent]\\n        if sents:\\n            normed_doc = '  '.join(sents)\\n        else:\\n            normed_doc = ' '\\n        normed_docs.append(normed_doc)\\n        \\n    fname_normed = fname[:-4] + '_normed.txt'\\n    with open(fname_normed, 'w', encoding='utf-8') as f:\\n        for doc in normed_docs:\\n            f.write('%s\\n'%doc)\\n    print('done with %s'%fname.split('/')[-1])\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from soynlp.hangle import normalize\n",
    "\n",
    "for fname in fnames:\n",
    "    with open(fname, encoding='utf-8') as f:\n",
    "        docs = [doc.strip() for doc in f]\n",
    "        \n",
    "    normed_docs = []\n",
    "    for doc in docs:\n",
    "        sents = doc.split('  ')\n",
    "        sents = [normalize(sent, number=True) for sent in sents]\n",
    "        sents = [sent for sent in sents if sent]\n",
    "        if sents:\n",
    "            normed_doc = '  '.join(sents)\n",
    "        else:\n",
    "            normed_doc = ' '\n",
    "        normed_docs.append(normed_doc)\n",
    "        \n",
    "    fname_normed = fname[:-4] + '_normed.txt'\n",
    "    with open(fname_normed, 'w', encoding='utf-8') as f:\n",
    "        for doc in normed_docs:\n",
    "            f.write('%s\\n'%doc)\n",
    "    print('done with %s'%fname.split('/')[-1])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:27:57.521246Z",
     "start_time": "2018-03-07T13:27:57.510001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10-22_article_all_normed.txt\n",
      "/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10-23_article_all_normed.txt\n",
      "/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10-25_article_all_normed.txt\n",
      "/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10-24_article_all_normed.txt\n",
      "/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10-20_article_all_normed.txt\n",
      "/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10-26_article_all_normed.txt\n",
      "/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10-27_article_all_normed.txt\n",
      "/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10-28_article_all_normed.txt\n",
      "/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10-21_article_all_normed.txt\n",
      "/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10-29_article_all_normed.txt\n"
     ]
    }
   ],
   "source": [
    "fnames = glob.glob('/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10*_article_all_normed.txt')\n",
    "for fname in fnames:\n",
    "    print(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre_normalize vs normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:28:00.247117Z",
     "start_time": "2018-03-07T13:27:58.723628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26368 2016-10-24_article_all.txt\n",
      "8992 2016-10-29_article_all.txt\n",
      "24109 2016-10-21_article_all.txt\n",
      "28917 2016-10-27_article_all.txt\n",
      "12160 2016-10-23_article_all.txt\n",
      "22754 2016-10-28_article_all.txt\n",
      "28621 2016-10-26_article_all.txt\n",
      "30091 2016-10-20_article_all.txt\n",
      "26753 2016-10-25_article_all.txt\n",
      "9306 2016-10-22_article_all.txt\n"
     ]
    }
   ],
   "source": [
    "fnames = glob.glob('/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10*_article_all.txt')\n",
    "for fname in fnames:\n",
    "    with open(fname, encoding='utf-8') as f:\n",
    "        docs = [doc.strip() for doc in f]\n",
    "    print(len(docs), fname.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:28:01.805483Z",
     "start_time": "2018-03-07T13:28:00.589260Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9306 2016-10-22_article_all_normed.txt\n",
      "12160 2016-10-23_article_all_normed.txt\n",
      "26753 2016-10-25_article_all_normed.txt\n",
      "26368 2016-10-24_article_all_normed.txt\n",
      "30091 2016-10-20_article_all_normed.txt\n",
      "28621 2016-10-26_article_all_normed.txt\n",
      "28917 2016-10-27_article_all_normed.txt\n",
      "22754 2016-10-28_article_all_normed.txt\n",
      "24109 2016-10-21_article_all_normed.txt\n",
      "8992 2016-10-29_article_all_normed.txt\n"
     ]
    }
   ],
   "source": [
    "fnames = glob.glob('/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10*_article_all_normed.txt')\n",
    "for fname in fnames:\n",
    "    with open(fname, encoding='utf-8') as f:\n",
    "        docs = [doc.strip() for doc in f]\n",
    "    print(len(docs), fname.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각각의 일단위로 수집된 뉴스기사에서 normalize이전과 이후의 차이를 확인하기 위해 문장의 갯수가 같은지 확인함\n",
    "- 이상X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cohesion_Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:28:02.197591Z",
     "start_time": "2018-03-07T13:28:02.135357Z"
    }
   },
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    def __init__(self, fname, iter_sent=False):\n",
    "        self.fname = fname\n",
    "        self.iter_sent = iter_sent\n",
    "        self.doc_length = 0\n",
    "        self.sent_length = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        with open(self.fname, encoding='utf-8') as f:\n",
    "            for doc in f:\n",
    "                doc = doc.strip()\n",
    "                # 핵심영역\n",
    "                if not self.iter_sent:\n",
    "                    yield doc\n",
    "                    continue\n",
    "                for sent in doc.split('  '):\n",
    "                    yield sent\n",
    "                    \n",
    "    def __len__(self):\n",
    "        if self.iter_sent:\n",
    "            if self.sent_length == 0:\n",
    "                with open(self.fname, encoding='utf-8') as f:\n",
    "                    for doc in f:\n",
    "                        self.sent_length += len(doc.strip().split('  '))\n",
    "            return self.sent_length\n",
    "        else:\n",
    "            if self.doc_length == 0:\n",
    "                with open(self.fname, encoding='utf-8') as f:\n",
    "                    for num_doc, doc in enumerate(f):\n",
    "                        continue\n",
    "                    self.doc_length = (num_doc + 1)\n",
    "            return self.doc_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:28:02.531675Z",
     "start_time": "2018-03-07T13:28:02.523032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ...\n",
      "\n",
      "위키리크스 클린턴 전 대통령 최측근 메모 공개  워싱턴 연합뉴스 신지홍 특파원 미국 민주당 대선후보 힐러리 클린턴의 남편인 빌 클린턴 전 대통령이 자신이 고문으로 속한 한 기업을  ...\n",
      "\n",
      "워싱턴 연합뉴스 심인성 특파원 미국 공화당 대선후보 도널드 트럼프가 26일 현지시간 민주당 지도부와 민주당 대선후보 힐러리 클린턴 캠프 내부 이메일해킹 사건의 배후와 관련해 북한이 ...\n",
      "\n",
      "로마 연합뉴스 현윤경 특파원 26일 밤 이탈리아 중부 산악 지대를 규모 5 4와 5 9의 지진이 잇따라 강타한 가운데 살인범을 포함한 흉악범 3명이 지진을 틈타 로마 외곽의 교도소 ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_fname = '/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10-28_article_all_normed.txt'\n",
    "corpus = Corpus(corpus_fname, iter_sent=False)\n",
    "\n",
    "for num_doc, doc in enumerate(corpus):\n",
    "    if num_doc >= 4: break\n",
    "    print('%s ...\\n'%doc[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:28:02.928551Z",
     "start_time": "2018-03-07T13:28:02.798311Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 115 ms, sys: 11.9 ms, total: 126 ms\n",
      "Wall time: 125 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22754"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:28:03.171612Z",
     "start_time": "2018-03-07T13:28:03.168109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 6.68 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22754"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iter_sent = True로 변경하면 for loop에 의하여 출력되는 객체가 문서가 아닌 문장임을 확인할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:28:03.442243Z",
     "start_time": "2018-03-07T13:28:03.433792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent 0 :  \n",
      "\n",
      "sent 1 : 위키리크스 클린턴 전 대통령 최측근 메모 공개 \n",
      "\n",
      "sent 2 : 워싱턴 연합뉴스 신지홍 특파원 미국 민주당 대선후보 힐러리 클린턴의 남편인 빌 클린턴 전 대통령이 자신이 고문으로 속한 한 기업을 통해 고액강연을 주선 받거나 가족재단인 클린턴재단 에 수천만 달러의 기부금이 흘러들어가도록 한 것으로 밝혀졌다고 미 언론이 27일 현지시간 전했다 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus.iter_sent = True\n",
    "for num_sent, sent in enumerate(corpus):\n",
    "    if num_sent >= 3: break\n",
    "    print('sent %d : %s \\n'%(num_sent, sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohesion probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:28:03.705038Z",
     "start_time": "2018-03-07T13:28:03.699841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "ab\n",
      "abc\n",
      "abcd\n",
      "abcde\n"
     ]
    }
   ],
   "source": [
    "for e in range(1, 5+1):\n",
    "    print('abcdef'[:e])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enumerate는 list나 set과 같은 for loop안에 넣을 수 있는 데이터들의 원소 하나씩을 꺼내면서, 각 원소들이 몇 번째로 선택된건지에 대한 index를 출력해주는 함수임. 즉, 아래와 같은 코드를 쓰지 않아도 됨. 이후에 zip 등과 같은 기능과 함께 섞어 쓰면 매우 편리하니 기억하는 것을 권장함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:28:04.023920Z",
     "start_time": "2018-03-07T13:28:03.959518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 a\n",
      "1 b\n",
      "2 c\n",
      "3 d\n"
     ]
    }
   ],
   "source": [
    "test_list = ['a','b','c','d']\n",
    "for i in range(len(test_list)):\n",
    "    print(i, test_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:28:04.811610Z",
     "start_time": "2018-03-07T13:28:04.383085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 a\n",
      "1 b\n",
      "2 c\n",
      "3 d\n"
     ]
    }
   ],
   "source": [
    "for i, element in enumerate(test_list):\n",
    "    print(i, element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sys.stdout.write('\\rYOUR MESSAGE')는 progress를 출력하는데 용이함\n",
    "Corpus를 돌면서 각 문장의 어절마다 Left-side subwords인 L의 빈도수를 defaultdict를 이용하여 카운트함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:28:12.767136Z",
     "start_time": "2018-03-07T13:28:05.149911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserting subwords into L: done\n",
      "num subword = 578867\n",
      "num subword = 30922 (after pruning with min count 30)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "corpus.iter_sent = True\n",
    "max_l_length = 10\n",
    "min_count = 30\n",
    "\n",
    "L = defaultdict(lambda: 0)\n",
    "\n",
    "for num_sent, sent in enumerate(corpus):\n",
    "    if num_sent % 5000 == 0:\n",
    "        sys.stdout.write('\\risnerting %d sents... ' % num_sent)\n",
    "    for token in sent.split():\n",
    "        for e in range(1, min(max_l_length, len(token)) + 1):\n",
    "            subword = token[:e]\n",
    "            L[subword] += 1\n",
    "print('\\rinserting subwords into L: done')\n",
    "print('num subword = %d' % len(L))\n",
    "\n",
    "L = {subword:freq for subword, freq in L.items() if freq >= min_count}\n",
    "print('num subword = %d (after pruning with min count %d)' % (len(L), min_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cohesion은 길이가 2이상인 subword에 대해 정의가 되기 때문에 길이가 1인 단어에 대해서는 1.0을 리턴한다\n",
    "\n",
    "또한 word가 L에 없는 경우 (빈도수가 min_count 이하이거나 아예 코퍼스에 등장하지 않았던 경우)에는 0.0을 return하는 예외 처리를 설정\n",
    "\n",
    "cohesion은 결국 (word의 빈도수 / 맨 왼쪽의 글자 빈도수)의 1/(n-1)승 임. numpy.power.는 시수승 계산을 할 수 있도록 도와줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:28:12.984794Z",
     "start_time": "2018-03-07T13:28:12.982810Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(L) # {'위': 17603, '위키': 120, ...} 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:28:13.248332Z",
     "start_time": "2018-03-07T13:28:13.197899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "청와대 :  0.722212060689\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_cohesion(word):\n",
    "    # 글자가 아니거가 공백, 혹은 희귀한 단어인 경우\n",
    "    if(not word) or((word in L) == False):\n",
    "        return 0.0\n",
    "    \n",
    "    if len(word) == 1:\n",
    "        return 1.0\n",
    "    \n",
    "    word_freq = L.get(word, 0)\n",
    "    base_freq = L.get(word[:1], 0)\n",
    "    \n",
    "    if base_freq == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return np.power((word_freq / base_freq), 1/(len(word) - 1))\n",
    "    \n",
    "print('청와대 : ', get_cohesion('청와대'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "길이가 2이상인 subwords에 대하여 cohesion을 미리 계산해 둔다. 앞에서 min_count로 피러링을 한 번 했기 때문에 적은 개수의 subwords만 cohesion을 계산했음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:28:13.731086Z",
     "start_time": "2018-03-07T13:28:13.465969Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29908"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohesion_dict = {word: get_cohesion(word) for word in L if len(word)>=2}\n",
    "len(cohesion_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예시 단어들의 cohesion을 실제로 계산해보자. L + [R]의 경계가 되는 지점들에서 cohesion의 값들이 하락함을 확인할 수 있음\n",
    "\n",
    "청와라는 글자가 등장하면 대부분 청와대가 등장했기 때문에 '청와대'의 cohesion이 '청와'의 cohesion보다 큼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:28:14.046111Z",
     "start_time": "2018-03-07T13:28:14.040538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "청와 = 0.522\n",
      "청와대 = 0.722\n",
      "청와대는 = 0.283\n",
      "민정수석 = 0.474\n",
      "민정수식이 = 0.000\n",
      "\n",
      "박근 = 0.294\n",
      "박근혜 = 0.539\n",
      "박근혜의 = 0.144\n"
     ]
    }
   ],
   "source": [
    "for word in ['청와', '청와대', '청와대는', '민정수석', '민정수식이', '', '박근', '박근혜', '박근혜의']:\n",
    "    if not word:\n",
    "        print()\n",
    "        continue\n",
    "        \n",
    "    print('%s = %.3f'%(word, cohesion_dict.get(word, 0.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어절 word가 입력되었을 때, L들에서 cohesion이 가장 높은 subword를 잘라내는 토크나이저를 만들어 보자\n",
    "\n",
    "길이가 2 이상일 떄 cohesion이 정의도기 때문에 길이가 2이하인 단어는 그래도 리턴\n",
    "\n",
    "subword의 ending point e는 길이가 2부터 'word의 길이 혹은 L의 최대 길이'의 min까지 임. range(b,e)에서 e는 포함되지 않기 때문에 +1을 해주는 것을 잊으면 안된다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:28:14.370204Z",
     "start_time": "2018-03-07T13:28:14.357946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "청와대 -> 청와대 \n",
      "\n",
      "청와대는 -> 청와대 \n",
      "\n",
      "민정수석 -> 민정수석 \n",
      "\n",
      "민정수석이 -> 민정수석 \n",
      "\n",
      "박근혜 -> 박근혜 \n",
      "\n",
      "박근혜의 -> 박근혜 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def L_tokenize(word):\n",
    "    if len(word) <= 2:\n",
    "        return word\n",
    "    \n",
    "    score = []\n",
    "    for e in range(2, min(len(word), max_l_length) + 1):\n",
    "        subword = word[:e]\n",
    "        score.append((subword, cohesion_dict.get(subword, 0), e)) # (word, cohesion, length)\n",
    "        \n",
    "    return sorted(score, key=lambda x:(x[1], x[2]), reverse=True)[0][0]\n",
    "\n",
    "for word in ['청와대', '청와대는', '민정수석', '민정수석이', '박근혜', '박근혜의']:\n",
    "    print('%s -> %s \\n'%(word, L_tokenize(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L_tokenizer는 상황에 맞게 튜닝할 수도 있음. Cohesion의 최대값을 지니는 subword를 선택하는 것이 아니라, 최대값과의 크기가 0.1이하로 차이가 나는 subword 중에서는 가장 긴 subword를 추출하고 싶다면 아래과 같이 tolerance를 argument로 넣을 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:28:15.486584Z",
     "start_time": "2018-03-07T13:28:14.682383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w tolerance : 기자단에 -> 기자\n",
      "wo tolerance : 기자단에 -> 기자단\n"
     ]
    }
   ],
   "source": [
    "my_cohesion_dict = {\n",
    "    '기자' : 0.5,\n",
    "    '기자단' : 0.45\n",
    "}\n",
    "\n",
    "def L_tokenize_w_tolerance(word, tolerance=0.1):\n",
    "    if len(word) <= 2:\n",
    "        return word\n",
    "    \n",
    "    scores = []\n",
    "    for e in range(2, min(len(word), max_l_length)+1):\n",
    "        subword = word[:e]\n",
    "        scores.append((subword, my_cohesion_dict.get(subword, 0))) # (word, cohesion)\n",
    "        \n",
    "    max_score = max([score for subword, score in scores])\n",
    "    scores = [(subword, score) for (subword, score) in scores if (max_score - score) <= tolerance]\n",
    "    \n",
    "    return sorted(scores, key=lambda x:len(x[0]), reverse=True)[0][0]\n",
    "\n",
    "for word in ['기자단에']:\n",
    "    print('w tolerance : %s -> %s'%(word, L_tokenize_w_tolerance(word, tolerance=0)))\n",
    "    print('wo tolerance : %s -> %s'%(word, L_tokenize_w_tolerance(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Making py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:28:15.793281Z",
     "start_time": "2018-03-07T13:28:15.790819Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus_fname = '/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10-28_article_all_normed.txt'\n",
    "max_l_length = 10\n",
    "min_count = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:28:16.103464Z",
     "start_time": "2018-03-07T13:28:16.100641Z"
    }
   },
   "outputs": [],
   "source": [
    "from corpus import Corpus\n",
    "from cohesion import CohesionProbability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:28:16.540423Z",
     "start_time": "2018-03-07T13:28:16.401782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ...\n",
      "\n",
      "위키리크스 클린턴 전 대통령 최측근 메모 공개  워싱턴 연합뉴스 신지홍 특파원 미국 민주당 대선후보 힐러리 클린턴의 남편인 빌 클린턴 전 대통령이 자신이 고문으로 속한 한 기업을  ...\n",
      "\n",
      "워싱턴 연합뉴스 심인성 특파원 미국 공화당 대선후보 도널드 트럼프가 26일 현지시간 민주당 지도부와 민주당 대선후보 힐러리 클린턴 캠프 내부 이메일해킹 사건의 배후와 관련해 북한이 ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(corpus_fname, iter_sent=False)\n",
    "\n",
    "for num_doc, doc in enumerate(corpus):\n",
    "    if num_doc >= 3: break\n",
    "    print(\"%s ...\\n\"%doc[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:28:24.405096Z",
     "start_time": "2018-03-07T13:28:16.853715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserting subwords into L: done\n",
      "num subword = 578867\n",
      "num subword = 30922 (after pruning with min count 30)\n"
     ]
    }
   ],
   "source": [
    "corpus.iter_sent = True\n",
    "cohesion_probability = CohesionProbability()\n",
    "cohesion_probability.train(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:28:24.720071Z",
     "start_time": "2018-03-07T13:28:24.713020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "청와 = 0.522\n",
      "청와대 = 0.722\n",
      "청와대는 = 0.283\n",
      "민정수석 = 0.474\n",
      "민정수석이 = 0.313\n",
      "박근 = 0.294\n",
      "박근혜 = 0.539\n",
      "박근혜의 = 0.144\n"
     ]
    }
   ],
   "source": [
    "for word in ['청와', '청와대', '청와대는', '민정수석', '민정수석이', '박근', '박근혜', '박근혜의']:\n",
    "    if not word:\n",
    "        print()\n",
    "        continue\n",
    "        \n",
    "    print(\"%s = %.3f\"%(word, cohesion_probability.get_cohesion(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다른 폴더에 py파일을 넣어두었으면 sys.path.append()를 이용해서 해당 폴더의 위치를 넣어주면 됨. 그러면 파일을 이용할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:28:25.031279Z",
     "start_time": "2018-03-07T13:28:25.028779Z"
    }
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('../yourpackage/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Braching_Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:28:26.749340Z",
     "start_time": "2018-03-07T13:28:25.648709Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus_fname = '/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10-28_article_all_normed.txt'\n",
    "\n",
    "max_l_length = 6\n",
    "max_r_length = 5\n",
    "min_count = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:28:27.257582Z",
     "start_time": "2018-03-07T13:28:27.070812Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176597"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from corpus import Corpus\n",
    "\n",
    "# True로 설정하면 문장를 기준으로 자료구조가 변경됨\n",
    "corpus = Corpus(corpus_fname, iter_sent=True)\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Branching Entropy\n",
    "Branching Entropy는 문장 단위로 for loop를 돌면서 subword가 어떻게 branching하는지 카운트를 하면 됨\n",
    "\n",
    "L은 '어린 --> 어린이'처럼 오른쪽에 어떤 글자가 등장하는지 카운트하는 dict_dict임. R은 '어린이 <-- 린이'처럼 왼쪽에 어떤 글자가 등장하는지 카운트하는 dict_dict임\n",
    "\n",
    "글자의 오른쪽 기준으로 subword를 자르기 위해서는 slice를 -로 하면 편함. 'abcde'에서 오른쪽 'de'를 가지고 오기 위해서는 'abcde'[-2:]를 하면 뒤에서 2번째 글자부터 쭉 가져오라는 의미임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:28:27.565703Z",
     "start_time": "2018-03-07T13:28:27.562613Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'abcde'[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:28:27.875979Z",
     "start_time": "2018-03-07T13:28:27.871702Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e\n",
      "de\n",
      "cde\n",
      "bcde\n"
     ]
    }
   ],
   "source": [
    "for b in range(1, 5):\n",
    "    print('abcde'[-b:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 이용하여 branching entropy를 학습하기 위한 from_word -> to_word: frequency를 저장하는 dict_dict를 만들어보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:28:46.528112Z",
     "start_time": "2018-03-07T13:28:28.183825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserting 175000 sents...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "L = defaultdict(lambda : defaultdict(lambda: 0)) # 어린 -> 어린이\n",
    "R = defaultdict(lambda : defaultdict(lambda: 0)) # 어린이 <- 린이\n",
    "\n",
    "for num_sent, sent in enumerate(corpus):\n",
    "    if num_sent % 5000 == 0:\n",
    "        sys.stdout.write('\\rinserting %d sents...'%num_sent)\n",
    "        \n",
    "    for token in sent.split():\n",
    "        if len(token) < 2:\n",
    "            continue\n",
    "            \n",
    "        for e in range(2, min(max_l_length, len(token)) + 1):\n",
    "            subword_from = token[:e-1]\n",
    "            subword_to = token[:e]\n",
    "            L[subword_from][subword_to] += 1\n",
    "            \n",
    "        for b in range(2, min(max_r_length, len(token)) + 1):\n",
    "            subword_from = token[-b+1:]\n",
    "            subword_to = token[-b:]\n",
    "            R[subword_from][subword_to] += 1\n",
    "            \n",
    "print('\\ndone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "av_I = 0 if not word in R else len(R[word])는 word가 R에 존재하지 않는 경우에는 0을 할당하고, word가 R에 존재한다면 R[word]를 가져온 뒤, 해당 dict의 길이를 av_l값으로 할당하라는 의미임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:28:50.236403Z",
     "start_time": "2018-03-07T13:28:50.228556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "박근\t(0, 7)\n",
      "박근혜\t(7, 23)\n",
      "국방\t(2, 24)\n",
      "국방부\t(1, 13)\n",
      "국방부는\t(0, 0)\n",
      "국방장\t(0, 1)\n",
      "국방장관\t(0, 7)\n",
      "트와이\t(0, 1)\n",
      "트와이스\t(0, 8)\n"
     ]
    }
   ],
   "source": [
    "def get_accessor_variety(word):\n",
    "    # av_l : ?-린이\n",
    "    # av_r : 어린-?\n",
    "    av_l = 0 if not word in R else len(R[word])\n",
    "    av_r = 0 if not word in L else len(L[word])\n",
    "    return (av_l, av_r)\n",
    "\n",
    "for word in ['박근', '박근혜', '국방', '국방부', '국방부는', '국방장', '국방장관', '트와이', '트와이스']:\n",
    "    av = get_accessor_variety(word)\n",
    "    print('%s\\t(%d, %d)'%(word, av[0], av[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Branching Entropy는 단어의 경계 부분에서 entropy가 증가함\n",
    "\n",
    "    국방\t(-0.000, 1.097)\n",
    "    국방부\t(-0.000, 1.601)\n",
    "    국방부는\t(0.000, 0.000)\n",
    "    국방장\t(0.000, -0.000)\n",
    "    국방장관\t(0.000, 1.575)\n",
    "    \n",
    "'국방' 다음에는 여러 단어가 올 수 있기 때문에 right-side entropy가 1.097임. 하지만 '국방장' 다음에는 반드시 '-관'이 나타나서 '국방장관'이 되었기 때문에 right-side entropy가 0임을 볼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:28:53.913364Z",
     "start_time": "2018-03-07T13:28:53.881239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "박근\t(0.000, 0.082)\n",
      "박근혜\t(1.510, 2.305)\n",
      "박근령\t(0.000, 1.335)\n",
      "국방\t(0.410, 1.097)\n",
      "국방부\t(-0.000, 1.601)\n",
      "국방부는\t(0.000, 0.000)\n",
      "국방장\t(0.000, -0.000)\n",
      "국방장관\t(0.000, 1.575)\n",
      "트와이\t(0.000, -0.000)\n",
      "트와이스\t(0.000, 1.313)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_branching_entropy(word):\n",
    "    def entropy(extensions):\n",
    "        '''extensions: dict[str]: int\n",
    "        '''\n",
    "        sum_ = sum(extensions.values())\n",
    "        if sum_ == 0:\n",
    "            return 0\n",
    "        \n",
    "        entropy = 0\n",
    "        for v in extensions.values():\n",
    "            if v == 0: continue\n",
    "            prob = v / sum_\n",
    "            entropy += (prob * np.log(prob))\n",
    "        return -1 * entropy\n",
    "\n",
    "    # be_l: ?-린이\n",
    "    # be_r: 어린-?\n",
    "\n",
    "    be_l = 0 if not word in R else entropy(R[word])\n",
    "    be_r = 0 if not word in L else entropy(L[word])    \n",
    "    \n",
    "    return (be_l, be_r)\n",
    "\n",
    "for word in ['박근', '박근혜', '박근령', '국방', '국방부', '국방부는', '국방장', '국방장관', '트와이', '트와이스']:\n",
    "    be = get_branching_entropy(word)\n",
    "    print('%s\\t(%.3f, %.3f)' % (word, be[0], be[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy가 높은 subword는 단어일 가능성이 높음. 좌/우에 등장할 다른 단어들의 종류가 다양하기 때문임\n",
    "\n",
    "그렇기 때문에 길이가 1인 글자의 entropy는 항상 높음. 길이가 1인 L들은 큰 의미를 지니 못하는 경향이 있음. 또한 의미를 알아볼 수 있는 길이가 1인 L의 단어는 많지 않음. 하지만 조사/어미 같은 경우에는 길이가 1인 경우가 많음. 그래서 left-side entropy 역시 매우 높게 나타는 경향이 존재함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:28:57.534280Z",
     "start_time": "2018-03-07T13:28:57.522852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "은\t(4.932, 2.379)\n",
      "는\t(3.879, 1.895)\n",
      "이\t(5.057, 4.214)\n",
      "가\t(4.471, 3.724)\n",
      "에게\t(3.583, 1.024)\n",
      "에서\t(4.896, 0.539)\n"
     ]
    }
   ],
   "source": [
    "for word in ['은', '는', '이', '가', '에게', '에서']:\n",
    "    be = get_branching_entropy(word)\n",
    "    print('%s\\t(%.3f, %.3f)'%(word, be[0], be[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer with Cohesion and Branching entropy\n",
    "Cohesion probability는 interior word scoring방법이며, Branching entropy는 exterior word scoring방법임 L_tokenizer를 cohesion만의 관점이 아니라 cohesion과 branching entropy를 함께 이용할 수도 있음. \n",
    "- 단어의 경계가 더 강조될 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:29:08.651807Z",
     "start_time": "2018-03-07T13:29:01.120749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserting subwords into L: done\n",
      "num subword = 578867\n",
      "num subword = 30922 (after pruning with min count 30)\n"
     ]
    }
   ],
   "source": [
    "from cohesion import CohesionProbability\n",
    "cohesion_probability = CohesionProbability()\n",
    "cohesion_probability.train(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T12:13:36.606398Z",
     "start_time": "2018-03-07T12:13:36.603354Z"
    }
   },
   "source": [
    "sorted함수는 key를 두 개 이용할 수 있음\n",
    "    \n",
    "    key=lambda x:(x[1], x[2])\n",
    "    \n",
    "위 구문은 x라는 item이 들어오면 1차 기준으로 x[1]을 이용하고, x[1]이 같을 경우 x[2]를 이용하라는 의미임. reverse=True이므로 x[1]이 큰 순서대로 정렬되며, x[1]이 같을 경우에는 x[2]가 같은 순서대로 정렬됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:29:12.253387Z",
     "start_time": "2018-03-07T13:29:12.237252Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "청와대 --> 청와대\n",
      "\n",
      "청와대는 --> 청와대\n",
      "\n",
      "민정수석 --> 민정수석\n",
      "\n",
      "민정수석이 --> 민정수석\n",
      "\n",
      "공연을 --> 공연\n",
      "\n",
      "공연하게 --> 공연\n",
      "\n",
      "박근혜 --> 박근혜\n",
      "\n",
      "박근혜의 --> 박근혜\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def cpbe_tokenize(word):\n",
    "    if (not word) or (len(word) <= 2):\n",
    "        return word\n",
    "    \n",
    "    score = []\n",
    "    for e in range(2, min(len(word), max_l_length) + 1):\n",
    "        subword = word[:e]\n",
    "        be_l, be_r = get_branching_entropy(subword)\n",
    "        cp_l = cohesion_probability.get_cohesion(subword)\n",
    "        score.append((subword, cp_l * be_r, e)) # (word, cohesion * branching entropy, length)\n",
    "        \n",
    "    return sorted(score, key=lambda x:(x[1], x[1]), reverse=True)[0][0]\n",
    "\n",
    "for word in ['청와대', '청와대는', '민정수석', '민정수석이', '공연을', '공연하게', '박근혜', '박근혜의']:\n",
    "    print('%s --> %s\\n' % (word, cpbe_tokenize(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Noun extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:29:16.055942Z",
     "start_time": "2018-03-07T13:29:15.857952Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176597"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_fname = '/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10-28_article_all_normed.txt'\n",
    "\n",
    "max_l_length = 8\n",
    "max_r_length = 5\n",
    "min_count = 30\n",
    "\n",
    "noun_score_threshold = 0.1\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from corpus import Corpus\n",
    "\n",
    "corpus = Corpus(corpus_fname, iter_sent=True)\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L-R graph\n",
    "L-R graph는 L왼쪽에 어떤 R들이 등장하는지 확인하기 위한 그래프로 dict[L]에는 [R]들이 몇번 오른쪽에 등장하였는지의 빈도수임\n",
    "\n",
    "max_l_length 기준으로 subword_l를 잘랐으며, subword_r의 길이가 max_r_length보다 길 경우에는 lr_graph에 저장하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:29:29.814147Z",
     "start_time": "2018-03-07T13:29:19.649413Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_graph = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "for sent in corpus:\n",
    "    for token in sent.split():\n",
    "        for e in range(2, min(max_l_length, len(token)) + 1):\n",
    "            subword_l = token[:e]\n",
    "            subword_r = token[e:]\n",
    "            \n",
    "            if len(subword_r) > max_r_length:\n",
    "                continue\n",
    "            lr_graph[subword_l][subword_r] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defaultdict는 값을 추가할 때는 편리하지만, get할 때는 주의를 해야함. 존재하지 않는 key에 대하여 default value를 return하면서, 그 값을 dict에 저장함. defaultdict의 크기를 늘리지 않으면서 값을 가져오고 싶을 때에는 dict.get처럼 접근해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:29:36.685712Z",
     "start_time": "2018-03-07T13:29:36.679553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x7f8caedc5840>, {'a': 1})\n",
      "0\n",
      "defaultdict(<function <lambda> at 0x7f8caedc5840>, {'a': 1, 'b': 0})\n"
     ]
    }
   ],
   "source": [
    "_tmp = defaultdict(lambda: 0)\n",
    "_tmp['a'] = 1\n",
    "print(_tmp)\n",
    "\n",
    "print(_tmp['b'])\n",
    "print(_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L인 word가 주어지면 R들의 분포를 가져오기 위해 get함수를 이용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:29:43.512355Z",
     "start_time": "2018-03-07T13:29:43.503094Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "박근혜 - : 4683\n",
      "박근혜 - 의: 53\n",
      "박근혜 - 파들: 8\n",
      "박근혜 - 정부: 27\n",
      "박근혜 - 를: 66\n",
      "박근혜 - 는: 94\n",
      "박근혜 - 대통령: 12\n",
      "박근혜 - 와: 43\n",
      "박근혜 - 정권: 4\n",
      "박근혜 - 인가: 3\n",
      "박근혜 - 역적패당이: 1\n",
      "박근혜 - 정권을: 1\n",
      "박근혜 - 가: 32\n",
      "박근혜 - 씨가: 3\n",
      "박근혜 - 대통령이: 3\n",
      "박근혜 - 최순실: 1\n",
      "박근혜 - 라는: 11\n",
      "박근혜 - 이다: 1\n",
      "박근혜 - 정부에: 2\n",
      "박근혜 - 에게는: 1\n",
      "박근혜 - 정부의: 16\n",
      "박근혜 - 정부를: 4\n",
      "박근혜 - 에게: 10\n",
      "박근혜 - 정부는: 2\n",
      "박근혜 - 대통령의: 10\n",
      "박근혜 - 표: 1\n",
      "박근혜 - 정부도: 1\n",
      "박근혜 - 정부가: 14\n",
      "박근혜 - 란: 1\n",
      "박근혜 - 측: 1\n",
      "박근혜 - 와의: 4\n",
      "박근혜 - 도: 4\n",
      "박근혜 - 백: 1\n",
      "박근혜 - 정권에서: 1\n",
      "박근혜 - 정권의: 1\n",
      "박근혜 - 보다: 1\n",
      "박근혜 - 정권에: 1\n",
      "박근혜 - 에: 2\n",
      "박근혜 - 게이트: 1\n",
      "박근혜 - 파에서까지: 2\n",
      "박근혜 - 만: 1\n",
      "박근혜 - 정부에서의: 1\n",
      "박근혜 - 이: 1\n",
      "박근혜 - 대통령에게: 2\n",
      "박근혜 - 대통령은: 2\n",
      "박근혜 - 대통령과: 1\n"
     ]
    }
   ],
   "source": [
    "def get_r_features(word):\n",
    "    return lr_graph.get(word, {})\n",
    "\n",
    "word = '박근혜'\n",
    "for r, freq in get_r_features(word).items():\n",
    "    print('%s - %s: %d'%(word, r, freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. wordextraction_and_tokenization_with_soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:29:50.311640Z",
     "start_time": "2018-03-07T13:29:50.280634Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.301'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import soynlp\n",
    "soynlp.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DoublespaceLineCorpus\n",
    "- 하나의 어절을 기준으로 L+[R]을 토큰나이즈하는 것이 아닌 한 어절의 앞 어절의 마지막 글자까지 고려한 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:29:57.173290Z",
     "start_time": "2018-03-07T13:29:57.161570Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent 0: 위키리크스 클린턴 전 대통령 최측근 메모 공개 \n",
      "\n",
      "sent 1: 워싱턴 연합뉴스 신지홍 특파원 미국 민주당 대선후보 힐러리 클린턴의 남편인 빌 클린턴 전  ....\n",
      "\n",
      "sent 2: 클린턴 전 대통령이 그 대가로 어떤 도움을 줬는지는 알 수 없지만 공화당 대선후보 도널드  ....\n",
      "\n",
      "sent 3: 위키리크스가 공개한 클린턴 전 대통령의 측근 더글러스 밴드가 과거 기록한 12쪽짜리 메모에 ....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "\n",
    "corpus_fname = '/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10-28_article_all_normed.txt'\n",
    "corpus = DoublespaceLineCorpus(corpus_fname, iter_sent=True, num_sent = 4)\n",
    "\n",
    "for n_sent, sent in enumerate(corpus):\n",
    "    print('sent %d: %s %s\\n'%(n_sent, sent[:50], '' if len(sent) < 50 else '....'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:30:04.396644Z",
     "start_time": "2018-03-07T13:30:04.391628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs 0 (#sents=1):  \n",
      "\n",
      "docs 1 (#sents=11): 위키리크스 클린턴 전 대통령 최측근 메모 공개  워싱턴 연합뉴스 신지홍 특파원 미국 민주당 ...\n",
      "\n",
      "docs 2 (#sents=9): 워싱턴 연합뉴스 심인성 특파원 미국 공화당 대선후보 도널드 트럼프가 26일 현지시간 민주당 ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = DoublespaceLineCorpus(corpus_fname, iter_sent=False, num_doc=3)\n",
    "\n",
    "for n_doc, doc in enumerate(corpus):\n",
    "    print('docs %d (#sents=%d): %s %s\\n'%(n_doc, len(doc.split('  ')), doc[:50].strip(), '' if len(doc) < 50 else '...'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordExtractor(Cohesion score, Branching Entropy, Accessor Variety)\n",
    "wordExtractor는 Cohesion score, Branching Entropy, Accessor Variety 등을 한번에 계산할 수 있는 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:30:45.945527Z",
     "start_time": "2018-03-07T13:30:11.336332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.333 Gbse memory 1.277 Gb\n"
     ]
    }
   ],
   "source": [
    "from soynlp.word import WordExtractor\n",
    "\n",
    "corpus = DoublespaceLineCorpus(corpus_fname, iter_sent=True)\n",
    "word_extractor = WordExtractor(left_max_length=10, right_max_length=6, min_count = 5)\n",
    "word_extractor.train(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordExtractor.word_scores()를 하면, 모든 L,R의 subwords에 대하여 Cohesion score, Branching Entropy, Accessor Variety, frequency 등을 모두 계산하여 출력할 수 있음. 여기서 계산하는 Branching Entropy는 어절 간의 글자들도 고려한 수치임\n",
    "\n",
    "return type은 {word:namedtuple)형식. Python의 namedtuple형식이기 때문에 .을 이용해서 해당 값을 손쉽게 가져올 수 있음\n",
    "\n",
    "leftside_frequency는 해당 단어가 L에 등장한 횟수이며, rightside_frequency는 해당 단어가 R에 등장한 횟수임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:31:01.911427Z",
     "start_time": "2018-03-07T13:30:52.878822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all cohesion probabilities was computed. # words = 186939\n",
      "all branching entropies was computed # words = 300695\n",
      "all accessor variety was computed # words = 300695\n"
     ]
    }
   ],
   "source": [
    "scores = word_extractor.word_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:31:11.244443Z",
     "start_time": "2018-03-07T13:31:11.241295Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Scores(cohesion_forward=0.51729927444984114, cohesion_backward=0.22446560846560848, left_branching_entropy=3.051534233484362, right_branching_entropy=3.319489521747003, left_accessor_variety=148, right_accessor_variety=205, leftside_frequency=8627, rightside_frequency=5303)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores['뉴스']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:31:20.417691Z",
     "start_time": "2018-03-07T13:31:20.414814Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51729927444984114"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores['뉴스'].cohesion_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:31:29.769745Z",
     "start_time": "2018-03-07T13:31:29.766555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트와이 (2.397, -0.000)\n",
      "트와이스 (2.397, 2.703)\n"
     ]
    }
   ],
   "source": [
    "for word in ['트와이', '트와이스']:\n",
    "    print(word, '(%.3f, %.3f)'%(scores[word].left_branching_entropy, scores[word].right_branching_entropy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noun extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:32:00.970023Z",
     "start_time": "2018-03-07T13:31:39.201555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used default noun predictor; Sejong corpus predictor\n",
      "used noun_predictor_sejong\n",
      "2398 r features was loaded\n",
      "scanning completed\n",
      "(L,R) has (77450, 38430) tokens\n",
      "building lr-graph completed"
     ]
    }
   ],
   "source": [
    "from soynlp.noun import LRNounExtractor\n",
    "\n",
    "corpus = DoublespaceLineCorpus(corpus_fname, iter_sent=True)\n",
    "noun_extractor = LRNounExtractor()\n",
    "nouns = noun_extractor.train_extract(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:32:10.536304Z",
     "start_time": "2018-03-07T13:32:10.531906Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "박근 is noun? False\n",
      "\n",
      "박근혜 is noun? True\n",
      "\n",
      "대통령 is noun? True\n",
      "\n",
      "정 is noun? True\n",
      "\n",
      "정부 is noun? True\n",
      "\n",
      "정부의 is noun? False\n",
      "\n",
      "알아 is noun? False\n",
      "\n",
      "알아냈 is noun? False\n",
      "\n",
      "트와이스 is noun? True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word in ['박근', '박근혜', '대통령', '정', '정부', '정부의', '알아', '알아냈', '트와이스']:\n",
    "    print('%s is noun? %r\\n'%(word, noun_extractor.is_noun(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:32:19.882491Z",
     "start_time": "2018-03-07T13:32:19.828988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['태극기', '집회에', '대한', '조사', '가', '시작되었다']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from soynlp.tokenizer import LTokenizer, MaxScoreTokenizer, RegexTokenizer\n",
    "\n",
    "cohesion_scores = {word:score.cohesion_forward for word, score in scores.items()}\n",
    "ltokenizer = LTokenizer(scores=cohesion_scores)\n",
    "\n",
    "ltokenizer.tokenize('태극기 집회에 대한 조사가 시작되었다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:32:29.893523Z",
     "start_time": "2018-03-07T13:32:29.890353Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['촛불', '집회에', '대한', '조사', '가', '시작되었다']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltokenizer.tokenize('촛불집회에 대한 조사가 시작되었다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:32:39.990181Z",
     "start_time": "2018-03-07T13:32:39.986765Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('촛불', '집회에'), ('대한', ''), ('조사', '가'), ('시작되었다', '')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltokenizer.tokenize('촛불집회에 대한 조사가 시작되었다', flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:32:49.782672Z",
     "start_time": "2018-03-07T13:32:49.778554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['촛불', '집회에', '대한', '조사', '가', '시작되었다']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltokenizer.tokenize('촛불집회에 대한 조사가 시작되었다', flatten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:32:59.717447Z",
     "start_time": "2018-03-07T13:32:59.713144Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['박근혜', '게이트', '에', '대한', '조사', '가', '시작', '되었습니다']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltokenizer.tokenize('박근혜 게이트에 대한 조사가 시작되었습니다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:33:09.793390Z",
     "start_time": "2018-03-07T13:33:09.790157Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['조원진', '이', '태극기', '집회를', '주도한', '다']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltokenizer.tokenize('조원진이 태극기 집회를 주도한다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:33:19.580276Z",
     "start_time": "2018-03-07T13:33:19.577096Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['촛불', '대한', '조사', '시작되었다']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltokenizer.tokenize('촛불집회에 대한 조사가 시작되었다', remove_r=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaxscoreTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:33:29.503155Z",
     "start_time": "2018-03-07T13:33:29.499270Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['박근혜', '게이트', '에', '대한', '조사', '가', '시작', '되었다']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxscoretokenizer = MaxScoreTokenizer(scores=cohesion_scores)\n",
    "maxscoretokenizer.tokenize('박근혜 게이트에 대한 조사가 시작되었다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:33:39.507870Z",
     "start_time": "2018-03-07T13:33:39.504102Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('촛불', 0, 2, 0.99186991869918695, 2), ('집회가', 2, 5, 0.12723523242021731, 3)],\n",
       " [('과', 0, 1, 0, 1), ('격하게', 1, 4, 0.1733289313846238, 3)],\n",
       " [('변화', 0, 2, 0.27621776504297996, 2), ('되었다', 2, 5, 0.17110265312541781, 3)]]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxscoretokenizer.tokenize('촛불집회가 과격하게 변화되었다', flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:33:49.518154Z",
     "start_time": "2018-03-07T13:33:49.513017Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕', '123', 'abf', '!@$']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regextokenizer = RegexTokenizer()\n",
    "regextokenizer.tokenize('안녕123abf!@$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:33:59.667842Z",
     "start_time": "2018-03-07T13:33:59.664137Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ㅋㅋ뭔데ㅋㅋ어디가ㅠㅠㅇㅅㅇ'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from soynlp.tokenizer import normalize\n",
    "\n",
    "normalize('ㅋㅋㅋㅋㅋㅋㅋ뭔뎈ㅋㅋㅋㅋ어디가ㅠㅠㅠㅠㅠㅇㅅㅇ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (python3_0901)",
   "language": "python",
   "name": "python3_0901"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

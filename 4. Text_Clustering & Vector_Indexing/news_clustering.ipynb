{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하추치 뉴스를 분석할 수 있지만, 때로는 미리 분서하려는 모든 날의 뉴스로부터 단어사전을 만들어둘 필요가 있음. \n",
    "- 10일치의 뉴스에 대해서 명사를 모두 추출한 뒤, universal dictionary를 만드는 것\n",
    "- dictionary를 이용하여 2016-10-20 뉴스에 대해 뉴스의 군집화를 수행한 뒤, 각 군집의 키워드를 추출함으로 그날 뉴스의 핫키워드를 추출\n",
    "\n",
    "각 뉴스 문서 별로 term frquency matrix를 만들고, 이를 이용하여 곧바로 Regularized Logistic Regression을 통해 키워드를 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T06:43:54.813251Z",
     "start_time": "2018-03-15T06:43:54.807286Z"
    }
   },
   "outputs": [],
   "source": [
    "EXTRACT_NOUN_SET = True\n",
    "noun_dictionary_fname = '/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/models/noun_dictionary.txt'\n",
    "\n",
    "TOKENIZE_2016_1020 = True\n",
    "normed_corpus_fname = '/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10-20_article_all_normed.txt'\n",
    "tokenized_corpus_fname = '/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10-20_article_all_normed_nountokenized.txt'\n",
    "\n",
    "CREATE_TERM_FREQUENCY_MATRIX = True\n",
    "x_2016_1020_fname = '/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/models/2016-10-20_noun_x.mm'\n",
    "vectorizer_2016_1020_fname = '/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/models/2016-10-20_noun_vectorizer.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corpus_10days의 10일치 뉴스에 대해, 각 날의 기사마다 명사를 추출하여, 이를 nouns_dictionary에 누적하여 저장해보자\n",
    "\n",
    "아래의 구문을 통해 하루의 뉴스기사를 분석할 때마다, 명사사전의 크기가 얼마나 커져가는지도 확인할 수 있음\n",
    "\n",
    "피클링을 하지 않고 명사 사전을 텍스트 파일로 저장. 피클링은 데이터를 binary로 저장하기 때문에 파일로 직접 읽을 수가 없음. 하지만 텍스트 파일로 저장하면 눈으로 확인할 수 있고, 다른 프로그래밍언어로 읽을 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T06:46:33.838296Z",
     "start_time": "2018-03-15T06:43:55.532788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num corpus = 10\n",
      "used default noun predictor; Sejong corpus predictor\n",
      "used noun_predictor_sejong\n",
      "2398 r features was loaded\n",
      "scanning completed\n",
      "(L,R) has (35278, 17088) tokens\n",
      "building lr-graph completed\n",
      "corpus name = 2016-10-22_article_all_normed.txt, num nouns = 10468\n",
      "used default noun predictor; Sejong corpus predictor\n",
      "used noun_predictor_sejong\n",
      "2398 r features was loaded\n",
      "scanning completed\n",
      "(L,R) has (48684, 23844) tokens\n",
      "building lr-graph completed\n",
      "corpus name = 2016-10-23_article_all_normed.txt, num nouns = 16822\n",
      "used default noun predictor; Sejong corpus predictor\n",
      "used noun_predictor_sejong\n",
      "2398 r features was loaded\n",
      "scanning completed\n",
      "(L,R) has (88431, 43874) tokens\n",
      "building lr-graph completed\n",
      "corpus name = 2016-10-25_article_all_normed.txt, num nouns = 28596\n",
      "used default noun predictor; Sejong corpus predictor\n",
      "used noun_predictor_sejong\n",
      "2398 r features was loaded\n",
      "scanning completed\n",
      "(L,R) has (91374, 44957) tokens\n",
      "building lr-graph completed\n",
      "corpus name = 2016-10-24_article_all_normed.txt, num nouns = 35815\n",
      "used default noun predictor; Sejong corpus predictor\n",
      "used noun_predictor_sejong\n",
      "2398 r features was loaded\n",
      "scanning completed\n",
      "(L,R) has (93414, 45817) tokens\n",
      "building lr-graph completed\n",
      "corpus name = 2016-10-20_article_all_normed.txt, num nouns = 42636\n",
      "used default noun predictor; Sejong corpus predictor\n",
      "used noun_predictor_sejong\n",
      "2398 r features was loaded\n",
      "scanning completed\n",
      "(L,R) has (90928, 44978) tokens\n",
      "building lr-graph completed\n",
      "corpus name = 2016-10-26_article_all_normed.txt, num nouns = 47704\n",
      "used default noun predictor; Sejong corpus predictor\n",
      "used noun_predictor_sejong\n",
      "2398 r features was loaded\n",
      "scanning completed\n",
      "(L,R) has (91850, 45388) tokens\n",
      "building lr-graph completed\n",
      "corpus name = 2016-10-27_article_all_normed.txt, num nouns = 52429\n",
      "used default noun predictor; Sejong corpus predictor\n",
      "used noun_predictor_sejong\n",
      "2398 r features was loaded\n",
      "scanning completed\n",
      "(L,R) has (77450, 38430) tokens\n",
      "building lr-graph completed\n",
      "corpus name = 2016-10-28_article_all_normed.txt, num nouns = 55363\n",
      "used default noun predictor; Sejong corpus predictor\n",
      "used noun_predictor_sejong\n",
      "2398 r features was loaded\n",
      "scanning completed\n",
      "(L,R) has (76342, 37264) tokens\n",
      "building lr-graph completed\n",
      "corpus name = 2016-10-21_article_all_normed.txt, num nouns = 57779\n",
      "used default noun predictor; Sejong corpus predictor\n",
      "used noun_predictor_sejong\n",
      "2398 r features was loaded\n",
      "scanning completed\n",
      "(L,R) has (35933, 17534) tokens\n",
      "building lr-graph completed\n",
      "corpus name = 2016-10-29_article_all_normed.txt, num nouns = 58450\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "if EXTRACT_NOUN_SET:\n",
    "    import glob\n",
    "    from soynlp.noun import LRNounExtractor\n",
    "    from soynlp.utils import DoublespaceLineCorpus as Corpus\n",
    "    \n",
    "    corpus_fnames = glob.glob('/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/*_article_all_normed.txt')\n",
    "    print('num corpus = %d'%len(corpus_fnames))\n",
    "    \n",
    "    nouns_dictionary = set()\n",
    "    \n",
    "    for corpus_fname in corpus_fnames:\n",
    "        news_corpus = Corpus(corpus_fname, iter_sent=True)\n",
    "        noun_extractor = LRNounExtractor()\n",
    "        nouns = noun_extractor.train_extract(news_corpus)\n",
    "        \n",
    "        nouns_dictionary.update(set(nouns.keys()))\n",
    "        corpus_name = corpus_fname.split('/')[-1].split(')')[0]\n",
    "        print('\\ncorpus name = %s, num nouns = %d'%(corpus_name, len(nouns_dictionary)))\n",
    "        \n",
    "    with open(noun_dictionary_fname, 'w', encoding='utf-8') as f:\n",
    "        for noun in nouns_dictionary:\n",
    "            f.write('%s\\n'%noun)\n",
    "            \n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2016-10-20일 뉴스에 대하여 CountVectorizer를 이용하여 term frequency matrix를 만든 뒤, 그 날의 주요 뉴스들이 어떤 것들이 있었는지 군집화를 수행함. \n",
    "- KMeans는 sparse matrix로 군집화를 수행할 수 있음\n",
    "\n",
    "cutom_tokenizer를 이용함. noun_dictionary는 텍스트 파일로 저장되어 있으므로, 파일을 open할 때는 encoding='utf-8'로 설정해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T06:56:21.813590Z",
     "start_time": "2018-03-15T06:56:21.753597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['국정농단', '사태', '뉴스', '분석', '시작']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(noun_dictionary_fname, encoding='utf-8') as f:\n",
    "    nouns_dictionary = [noun.strip() for noun in f]\n",
    "    print(len(nouns_dictionary))\n",
    "    \n",
    "def custom_tokenize(doc):\n",
    "    def parse_noun(token):\n",
    "        for e in reversed(range(1, len(token) + 1)):\n",
    "            subword = token[:e]\n",
    "            if subword in nouns_dictionary:\n",
    "                return subword\n",
    "        return ''\n",
    "    \n",
    "    nouns = [parse_noun(token) for token in doc.split()]\n",
    "    nouns = [word for word in nouns if word]\n",
    "    return nouns\n",
    "\n",
    "custom_tokenize('국정농단의 사태에 대하여 뉴스 분석을 시작해봄')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2016-10-20 뉴스에 대하여 corpus를 만듦. corpus의 lenth로 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T09:42:47.221656Z",
     "start_time": "2018-03-15T06:57:25.338115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length of 2016-10-20 = 30091\n",
      "tokenization was done\n"
     ]
    }
   ],
   "source": [
    "if TOKENIZE_2016_1020:\n",
    "    import sys\n",
    "    # from corpus import Corpus\n",
    "    from soynlp.utils import DoublespaceLineCorpus as Corpus\n",
    "    corpus_2016_1020 = Corpus(normed_corpus_fname, iter_sent=False)\n",
    "    print('corpus length of 2016-10-20 = %d'%len(corpus_2016_1020))\n",
    "    \n",
    "    tokenized_corpus_2016_1020 = []\n",
    "    for num_doc, doc in enumerate(corpus_2016_1020):\n",
    "        if num_doc % 100 == 0:\n",
    "            sys.stdout.write('\\rtokenizing %d ...'%num_doc)\n",
    "        doc = ' '.join([noun for sent in doc.split('  ') for noun in custom_tokenize(sent)]).strip()\n",
    "        tokenized_corpus_2016_1020.append(doc)\n",
    "    print('\\rtokenization was done')\n",
    "    \n",
    "    with open(tokenized_corpus_fname, 'w', encoding='utf-8') as f:\n",
    "        for doc in tokenized_corpus_2016_1020:\n",
    "            f.write('%s\\n'%doc)\n",
    "            \n",
    "else:\n",
    "    with open(tokenized_corpus_fname, encoding='utf-8') as f:\n",
    "        tokenized_corpus_2016_1020 = [doc.strip() for doc in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer를 이용하여 term frequency matrix를 만듦. 만들어 둔 term frequency matrix는 corpus_10days/models/ 아래에 저장함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T09:46:36.221979Z",
     "start_time": "2018-03-15T09:46:28.051410Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30091, 2611)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "if CREATE_TERM_FREQUENCY_MATRIX:\n",
    "    vectorizer = CountVectorizer(min_df=0.005, max_df=0.8)\n",
    "    x_2016_1020 = vectorizer.fit_transform(tokenized_corpus_2016_1020)\n",
    "    print(x_2016_1020.shape)\n",
    "    \n",
    "    from scipy.io import mmwrite\n",
    "    mmwrite(x_2016_1020_fname, x_2016_1020)\n",
    "    \n",
    "    import pickle\n",
    "    with open(vectorizer_2016_1020_fname, 'wb') as f:\n",
    "        pickle.dump(vectorizer, f)\n",
    "        \n",
    "else:\n",
    "    from scipy.io import mmread\n",
    "    x_2016_1020 = mmread(x_2016_1020_fname).tocsr()\n",
    "    \n",
    "    import pickle\n",
    "    with open(vectorizer_2016_1020_fname, 'rb') as f:\n",
    "        vectorizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_2016\\_1020의 각 column에 해당하는 단어를 decoding하기 위하여 vectorizer.vocabulary_로부터 int2vocab을 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T09:48:06.898315Z",
     "start_time": "2018-03-15T09:48:06.891034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '01', '02', '03']\n"
     ]
    }
   ],
   "source": [
    "int2vocab = sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1])\n",
    "int2vocab = [word for word, idx in int2vocab]\n",
    "print(int2vocab[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만들어진 x_2016_1020을 이용하여 Spherical kmeans를 수행. 이를 위해 L2로 normalization을 수행한 뒤, 클러스터의 갯수를 1000개로 지정하여 k-means clustering을 수행함\n",
    "\n",
    "군집화나 토픽 모델링을 수행할 때는 예상하는 것보다 군집/토픽의 갯수를 크게 잡으면 됨. 중복되는 군집이 등장하면 묶으면 됨. 하지만, 데이터에 노이즈가 있으므로 다른 군집들이 하나의 군집으로 묶인다면 나중에 해석하기가 어려워짐\n",
    "\n",
    "특히, 여러 군집/토픽에서 두루두루 사용될 수 있는 단어들이 이러한 노이즈 역할을 함. 이런 단어들을 미리 걸러낼 수 있다면 훨씬 더 정교한 모델링이 될 것임(군집화나 토픽모델링에서는 불필요하다 생각되는 단어들을 과감하게 쳐낼수록 결과가 깔끔하게 나타나는 경향이 있음). 그렇지 않드면, 군집화/토픽모델링을 할 때 군집/토픽의 갯수를 크게 잡아주면 더 좋음\n",
    "\n",
    "군집화를 수행한 뒤, 20개의 군집들에 대해 각 군집에 할당된 뉴스의 갯수가 몇 개인지 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T09:56:33.019359Z",
     "start_time": "2018-03-15T09:53:47.480518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 16510.987\n",
      "Iteration  1, inertia 13134.890\n",
      "Iteration  2, inertia 12705.566\n",
      "Iteration  3, inertia 12504.660\n",
      "Iteration  4, inertia 12393.908\n",
      "Iteration  5, inertia 12329.651\n",
      "Iteration  6, inertia 12288.252\n",
      "Iteration  7, inertia 12258.259\n",
      "Iteration  8, inertia 12241.061\n",
      "Iteration  9, inertia 12231.583\n",
      "Iteration 10, inertia 12225.207\n",
      "Iteration 11, inertia 12220.059\n",
      "Iteration 12, inertia 12215.621\n",
      "Iteration 13, inertia 12212.095\n",
      "Iteration 14, inertia 12209.341\n",
      "Iteration 15, inertia 12207.791\n",
      "Iteration 16, inertia 12206.889\n",
      "Iteration 17, inertia 12206.405\n",
      "Iteration 18, inertia 12205.830\n",
      "Iteration 19, inertia 12205.528\n",
      "cluster # 0 has 34 docs\n",
      "cluster # 1 has 19 docs\n",
      "cluster # 2 has 22 docs\n",
      "cluster # 3 has 854 docs\n",
      "cluster # 4 has 59 docs\n",
      "cluster # 5 has 106 docs\n",
      "cluster # 6 has 52 docs\n",
      "cluster # 7 has 26 docs\n",
      "cluster # 8 has 46 docs\n",
      "cluster # 9 has 9 docs\n",
      "cluster # 10 has 1481 docs\n",
      "cluster # 11 has 156 docs\n",
      "cluster # 12 has 34 docs\n",
      "cluster # 13 has 280 docs\n",
      "cluster # 14 has 33 docs\n",
      "cluster # 15 has 52 docs\n",
      "cluster # 16 has 26 docs\n",
      "cluster # 17 has 27 docs\n",
      "cluster # 18 has 11 docs\n",
      "cluster # 19 has 31 docs\n",
      "CPU times: user 2min 57s, sys: 576 ms, total: 2min 57s\n",
      "Wall time: 2min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from collections import defaultdict\n",
    "\n",
    "x_2016_1020 = normalize(x_2016_1020, axis = 1, norm='l2')\n",
    "kmeans = KMeans(n_clusters=1000, max_iter=20, n_init=1, verbose=-1)\n",
    "clusters = kmeans.fit_predict(x_2016_1020)\n",
    "\n",
    "clusters_to_rows = defaultdict(lambda: [])\n",
    "for idx, label in enumerate(clusters):\n",
    "    clusters_to_rows[label].append(idx)\n",
    "    \n",
    "for i in range(20):\n",
    "    print('cluster # %d has %d docs'%(i, len(clusters_to_rows[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "군집 00번은 00개의 뉴스가 묶여 있음(군집화를 할 때마다 달라지므로, 다시 실행시킬 경우 다른 숫자로 나타날 것임). 이 군집에 대해 키워드를 추출하기 위해서는, 00번 군집에 해당하는 뉴스의 label을 1로, 다른 뉴스들을 -1로 둔 뒤, 해당 군집 00번을 구분할 수 있는 주요 단어들을 L1 regularized logistc regression을 이용하여 뽑아낼 수 있음\n",
    "\n",
    "y를 만든 뒤, 혹시 하는 마음에 x_2016_1020.shape과 같은지 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T10:17:42.011839Z",
     "start_time": "2018-03-15T10:17:41.953456Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30091, (30091, 2611))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_aspect = 15\n",
    "y = [1 if i in clusters_to_rows[cluster_aspect] else -1 for i in range(x_2016_1020.shape[0])]\n",
    "len(y), x_2016_1020.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression의 penalty를 'l1'으로 주고, Regularization cost를 1로 주었음. 키워드의 갯수가 너무 많으면 C를 더 적게, 키워드의 갯수가 너무 적다면 C를 좀 더 크게 조절할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T10:40:02.982200Z",
     "start_time": "2018-03-15T10:40:02.748723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "마이데일리 (20.572)\n",
      "사랑 (14.341)\n",
      "영화 (5.431)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_l1 = LogisticRegression(penalty='l1', C=1)\n",
    "logistic_l1.fit(x_2016_1020, y)\n",
    "\n",
    "keywords = sorted(enumerate(logistic_l1.coef_[0]), key=lambda x:x[1], reverse=True)[:30]\n",
    "for word, score in keywords:\n",
    "    if score == 0: break\n",
    "    print('%s (%.3f)' % (int2vocab[word], score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "관심있는 군집들에 대해 키워드를 뽑아내는 함수를 생성\n",
    "interested_clusters로 입력되는 클러스터들에 대하여 위처럼 각각의 클러스터에 대한\n",
    "Regularized Logistic Regression을 이용하여 키워드를 추출\n",
    "\n",
    "Regularization cost와 키워드의 갯수를 선택하기 위하여 print_keywords의 함수의 arguments에 이를 넣어둔다. L1 Logistic Regression에서 coefficient가 0이라는 것은 classification에서 유의미한 변수가 아니라는 뜻임. 그렇기 때문에 아래의 구문을 넣어서 coeffieicnt가 0보다 큰 단어들만 선택\n",
    "\n",
    "    keywords = [ int2vocab[word] for word, score in keywords if score > 0 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어 선택과 같은 튜닝 과정없이 명사 추출 + k-means clustering + 키워드 추출만으로도 어느 정도 그날의 뉴스 토픽들을 살펴볼 수 있음\n",
    "\n",
    "좀 더 정확한 단어를 선택하여 군집화가 잘 되게 만들고, 키워드를 추출하기에 좋은 parameter를 찾아야 함\n",
    "\n",
    "하지만 이러한 접근 방법에는 한 가지 단점이 존재함. 만약 18번 군집에 대한 키워드로 '디자이너'가 선택되었으나, 이 단어는 다른 군집에서의 키워드가 될 수 있음. 다른 여러개의 군집에서도 키워드로 사용될 수 있는 단어라면, 18번 군집과 비슷한 군집이 있기 때문임. 이는 topic modeling, doc2vec, 단어 선택을 통한 군집화의 고도화 등으로 해결해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T10:45:17.104752Z",
     "start_time": "2018-03-15T10:44:46.659478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cluster # 593 keywords (from 90 news)\n",
      " > ['투자', '글로벌', '상품', '미국', '설명', '국내', '유럽', '이상', '주식', '부동산']\n",
      "\n",
      "cluster # 440 keywords (from 80 news)\n",
      " > ['상승', '증시', '시장', '예상', '금리', '지수', '실적', '대비', '거래', '강세', '발표', '미국', '이후', '전일', '전망']\n",
      "\n",
      "cluster # 522 keywords (from 64 news)\n",
      " > ['대통령', '박근혜', '면서', '경북', '기록', '의장', '기존', '가방', '최순실', '지지율', '포인트', '서울신문']\n",
      "\n",
      "cluster # 135 keywords (from 63 news)\n",
      " > ['연합뉴스', '대전', '의료', '제공', '대구', '구조', '20일', '부산']\n",
      "\n",
      "cluster # 578 keywords (from 68 news)\n",
      " > ['북한', '우리', '송민순']\n",
      "\n",
      "cluster # 707 keywords (from 170 news)\n",
      " > ['미국', '소장', '세계', '것으', '박사', '뉴욕', '보도', '자동차', '차량', '분석', '가능성', '소송', '지난', '현대차', '세금', '유럽', '이후', '국내', '영국', '주택', '공급', '금지', '시장', '최대', '문제', '생산', '보고']\n",
      "\n",
      "cluster # 263 keywords (from 186 news)\n",
      " > ['경찰', '부검', '경찰관', '공격', '사건', '무단복제', '신고', '협의', '광주', '단체', '20대', '주장', '수사', '법원', '남성', '전국', '카드', '상황', '주변', '이런', '학교', '시스템', '차량', '관련', '총격', '사망', '시민들', '위반', '성능', '시민']\n",
      "\n",
      "cluster # 432 keywords (from 90 news)\n",
      " > ['대표', '창업', '제품', '개최', '다양', '공장', '준비', '회장', '개발', '배우', '도움', '사장', '중소기업', '올해']\n",
      "\n",
      "cluster # 533 keywords (from 168 news)\n",
      " > ['한국', '협력', '우리나라', '설명', '세계', '기여', '선정', '강화', '혁신', '참가', '성과', '건설', '정책', '아시아', '국가', '진출', '주요', '18일', '부문', '환경', '올해', '대회', '수요', '인정', '시장', '기업', '증가', '유지', '이라고', '주최']\n",
      "\n",
      "cluster # 75 keywords (from 53 news)\n",
      " > ['출석', '여야', '오늘', '국감', '불출석', '수석', '새누리당', '국회', '야당', '논의']\n",
      "\n",
      "cluster # 260 keywords (from 99 news)\n",
      " > ['작품', '작가', '전시', '소설', '전시회', '수상']\n",
      "\n",
      "cluster # 772 keywords (from 152 news)\n",
      " > ['서울', '공시', '서울신문', '일자리', '지원', '1천', '21일', '학교', '선정', '사이', '역사', '사실', '아예', '지난', '최근', '함께', '가장', '행사', '올해', '설치', '다양', '금융', '서울시', '전국', '기자', '공모']\n",
      "\n",
      "cluster # 998 keywords (from 57 news)\n",
      " > ['폭발', '사고', '터널', '차량', '공장', '발생', '작업', '있습니다']\n",
      "\n",
      "cluster # 856 keywords (from 102 news)\n",
      " > ['부산', '19일', '진행']\n",
      "\n",
      "cluster # 954 keywords (from 91 news)\n",
      " > ['제품', '온라인', '매출', '판매', '최근', '행사', '증정', '매장', '상품', '코리아', '구매', '필요', '인기', '다양', '때문', '가능', '가격', '것으']\n",
      "\n",
      "cluster # 287 keywords (from 53 news)\n",
      " > ['앨범', '발매', '활동', '정규', '11월', '팬들']\n",
      "\n",
      "cluster # 809 keywords (from 75 news)\n",
      " > ['교육', '대학', '대통령', '평가', '학생들', '대한민국', '학교', '학생', '운영']\n",
      "\n",
      "cluster # 388 keywords (from 115 news)\n",
      " > ['공연', '페스티벌', '함께', '이번', '축제', '최고', '아시아', '참여', '예정', '학생들']\n",
      "\n",
      "cluster # 6 keywords (from 52 news)\n",
      " > ['사고', '조사', '경찰', '진술', '김씨', '오전', '서울']\n",
      "\n",
      "cluster # 775 keywords (from 103 news)\n",
      " > ['실적', '연구', '이라고', '주가', '기대', '목표주', '개선', '영업이익', '내년', '투자', '투자증권', '3분기', '전망', '상승', '성장', '증권', '기자', '예상']\n",
      "\n",
      "cluster # 653 keywords (from 62 news)\n",
      " > ['총기', '인터넷', '사제총', '경찰관', '인터뷰', '앵커', '사제', '전자발찌', '사용']\n",
      "\n",
      "cluster # 704 keywords (from 208 news)\n",
      " > ['영화', '개봉', '순간', '배우', '관객', '시리즈', '감독', '연기', '촬영', '프랑스', '이야기', '대한', '생각', '번째', '아이들', '지난', '이들', '그들', '우리', '작품', '캐릭터', '관객들', '11월', '사람']\n",
      "\n",
      "cluster # 636 keywords (from 110 news)\n",
      " > ['혐의', '것으', '지난', '직장', '처리', '해상', '적발', '조사', '사실', '위반', '허위', '수사', '입건', '이상', '자리', '뉴스1', '이들', '경기', '집단', '구속', '자신', '광주', '경찰', '환자']\n",
      "\n",
      "cluster # 817 keywords (from 99 news)\n",
      " > ['일본', '방문', '참석', '경기', '관련', '세계', '해도', '아버지', '소송']\n",
      "\n",
      "cluster # 671 keywords (from 60 news)\n",
      " > ['울산', '태풍', '계획', '시장']\n",
      "\n",
      "cluster # 839 keywords (from 100 news)\n",
      " > ['서비스', '제공', '시장', '로봇', '지원', '세계', '확대', '페이스북', '기업', '정보', '공유', '인터넷', '업체', '플랫폼', '이용자', '예약', '적용', '이용', '운영', '도입', '있도록', '브랜드', '분야']\n",
      "\n",
      "cluster # 877 keywords (from 61 news)\n",
      " > ['제공', '연합뉴스', '20일', '서울', '미래창조과학부', '중국', '2016년']\n",
      "\n",
      "cluster # 648 keywords (from 77 news)\n",
      " > ['여성', '남성', '대한']\n",
      "\n",
      "cluster # 376 keywords (from 65 news)\n",
      " > ['가을', '축제', '코스', '다양', '여행', '함께', '위치', '특별', '운영']\n",
      "\n",
      "cluster # 66 keywords (from 139 news)\n",
      " > ['2016', '19', '24', '21', '23']\n",
      "\n",
      "cluster # 960 keywords (from 227 news)\n",
      " > ['방송', '시청자들', '프로그램', '전파', '도전', '채널', '자랑', '라디오', '포스터', '라이브', '배우', '전화', '아버지', '11월', '어떤', '헤럴드', '지금', '시즌', '드라마', '등장', '뷰티', '녹화', '지난', '모습', '이라고', '시작', '캡처', '화제', '자신', '함께']\n",
      "\n",
      "cluster # 641 keywords (from 372 news)\n",
      " > ['이번', '행사', '진행', '김수', '마련', '체험', '주제', '주최', '예정', '프로그램', '취업', '디자인', '참여', '운영', '참가', '관심', '공모', '제공', '무료', '문화', '협약', '지역', '홍보', '진출', '학생들', '경기', '대구', '희망', '생각', '참석']\n",
      "\n",
      "cluster # 882 keywords (from 64 news)\n",
      " > ['매각', '인수', '노조', '구조조정', '회사']\n",
      "\n",
      "cluster # 760 keywords (from 116 news)\n",
      " > ['중국', '때문', '인구', '미국', '경남', '여성들', '것으', '이름', '지난']\n",
      "\n",
      "cluster # 899 keywords (from 92 news)\n",
      " > ['기업', '30대', '지원', '강화', '발표', '대기업', '중소기업', '지급', '12', '개선', '대한']\n",
      "\n",
      "cluster # 537 keywords (from 102 news)\n",
      " > ['회장', '그룹', '사장', '매일신문', '대한', '대구', '경영']\n",
      "\n",
      "cluster # 908 keywords (from 213 news)\n",
      " > ['사업', '지원', '내년', '추진', '현대', '예산', '계획', '마을', '선정', '활성화', '조성', '구축', '지역', '지난', '설립', '필요', '확보', '경남', '어려움', '관계자는', '기대', '발굴', '성과', '설치', '국내', '수립', '개발', '인수']\n",
      "\n",
      "cluster # 544 keywords (from 71 news)\n",
      " > ['대출', '금리', '이하', '강화', '경우', '은행', '신청']\n",
      "\n",
      "cluster # 812 keywords (from 109 news)\n",
      " > ['라고', '이에', '방송', '많이', '우리', '20일', '이야기', '가방', '사실', '사진', '출연']\n",
      "\n",
      "cluster # 539 keywords (from 57 news)\n",
      " > ['선고', '기소', '범행', '시장', '상대', '혐의', '김씨', '재판', '피해자', '이씨', '편의점']\n",
      "\n",
      "cluster # 84 keywords (from 84 news)\n",
      " > ['발사', '실패', '추정', '무수단']\n",
      "\n",
      "cluster # 791 keywords (from 195 news)\n",
      " > ['정부', '대책', '부동산', '허용', '규제', '이상', '개선', '문제', '거래', '경제', '국회', '국감', '완화', '예산', '사용', '시장', '내용', '불법', '지역', '위기', '현재', '머니투데', '등록', '대한', '경기', '내년', '개정', '피해', '경우', '필요']\n",
      "\n",
      "cluster # 637 keywords (from 61 news)\n",
      " > ['검찰', '수사', '롯데', '사건', '부회장', '회장', '그룹', '부당', '기소']\n",
      "\n",
      "cluster # 393 keywords (from 52 news)\n",
      " > ['북한', '대변', '미사일', '발사', '규탄', '도발', '유엔']\n",
      "\n",
      "cluster # 997 keywords (from 59 news)\n",
      " > ['클린턴', '트럼프', '한국', '주장', '이에', '여성', '미국', '우리']\n",
      "\n",
      "cluster # 714 keywords (from 53 news)\n",
      " > ['교환', '삼성전자', '소비자', '갤럭시노트7', '장관']\n",
      "\n",
      "cluster # 515 keywords (from 144 news)\n",
      " > ['분양', '인근', '개통', '아파트', '위치', '단지', '설계', '이용', '입주', '가능', '구성', '지하', '규모', '지역', '예정', '프리미엄', '풍부', '공급', '조성', '때문', '84', '주택', '59']\n",
      "\n",
      "cluster # 896 keywords (from 104 news)\n",
      " > ['기술', '개발', '산업', '기능', '데이터', '필요', '업체', '시장', '분야', '인간', '11월']\n",
      "\n",
      "cluster # 428 keywords (from 57 news)\n",
      " > ['국정원장', '공개', '당시', '회고록', '국정원', '북한', '국감', '새누리당', '이병호', '국회', '송민순', '발언']\n",
      "\n",
      "cluster # 74 keywords (from 70 news)\n",
      " > ['차태', '연기', '서현진', '사랑', '영화', '라고', '작곡', '이형', '캐릭터']\n",
      "\n",
      "cluster # 123 keywords (from 61 news)\n",
      " > ['2015', '19']\n",
      "\n",
      "cluster # 838 keywords (from 53 news)\n",
      " > ['김유정', '차태', '라이브', '때문', '네이버']\n",
      "\n",
      "cluster # 845 keywords (from 151 news)\n",
      " > ['사진', '카메라', '소속사', '페이스북', '지난', '온라인', '가득', '시간', '최종', '모습', '데뷔', '당신', '유리', '여행', '촬영', '자랑', '일간스포츠', '미소', '하나', '공식', '기자', '팬들', '스타', '아름', '준비', '세계', '작품', '10월', '제공', '배우']\n",
      "\n",
      "cluster # 506 keywords (from 65 news)\n",
      " > ['홍나리', '촬영', '수애', '캐릭터', '생각', '소감', '인기', '남자', '드라마', '이하', '기자', '로맨틱', '조보아', '우리', '웹툰', '라고', '연하', '주연', '김정민']\n",
      "\n",
      "cluster # 165 keywords (from 54 news)\n",
      " > ['확산', '문화', '대통령', '기업들', '의혹', '위기', '설립', '이상', '재단']\n",
      "\n",
      "cluster # 559 keywords (from 132 news)\n",
      " > ['파워', '출연', '방송', '20일', '웃음', '이날', '이에', '가수', '대답', '질문', '게스트', '헤럴드', '면서', '많이', '활동', '일간스포츠', '에이핑크', '솔직', '라디오', '배우', '언급', '당시', '좋아', '무단']\n",
      "\n",
      "cluster # 25 keywords (from 121 news)\n",
      " > ['21', '2016']\n",
      "\n",
      "cluster # 355 keywords (from 69 news)\n",
      " > ['회고록', '문재인', '송민순', '기억', '새누리당', '하락', '입장', '의혹', '공세', '지지', '국민의당', '대표', '기록', '지지율', '대통령', '포인트']\n",
      "\n",
      "cluster # 652 keywords (from 62 news)\n",
      " > ['학생들', '총장', '의혹', '이대', '이화여대', '사퇴', '정유라', '학교']\n",
      "\n",
      "cluster # 737 keywords (from 263 news)\n",
      " > ['뉴시스', '진행', '운영', '이번', '지난', '기자', '전남', '20일', '지역', '전달', '올해', '실시', '개최', '시민', '선정', '대한', '경우', '방문', '수상', '활동', '있도록', '행사', '계획', '판매', '주제', '설치', '문화', '함께', '전시', '않은']\n",
      "\n",
      "cluster # 673 keywords (from 122 news)\n",
      " > ['출시', '애플', '국내', '브랜드', '업계', '시장', '모델', '음원', '관계자는', '지난', '건강', '판매', '시리즈', '경기', '가능', '함께', '세계', '기념', '번째', '개발', '다양', '확대', '것으', '최근', '사용']\n",
      "\n",
      "cluster # 462 keywords (from 70 news)\n",
      " > ['여의', '뉴시스', '국회', '76', '사진', '83', '서울']\n",
      "\n",
      "cluster # 105 keywords (from 211 news)\n",
      " > ['사진', '제공', '뉴시스']\n",
      "\n",
      "cluster # 786 keywords (from 271 news)\n",
      " > ['서울', '사진', '이종', '제보', '앞둔', '뉴시스', '서비스', '공공기관', '전국', '경찰', '인사말', '영상', '서울시', '애플']\n",
      "\n",
      "cluster # 93 keywords (from 52 news)\n",
      " > ['뉴시스', '공시', '서울', '뉴시스통신사', '계약']\n",
      "\n",
      "cluster # 421 keywords (from 51 news)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > ['청와대', '거부', '이유', '야당', '사유서', '불출석', '수석', '국회', '대한', '운영위', '국정감사', '국감']\n",
      "\n",
      "cluster # 538 keywords (from 88 news)\n",
      " > ['기자간담회', '대한', '지난', '방문', '뉴시스', '트레', '오전', '제보', '규모', '영상', '사장', '사무실', '검찰', '의혹', '서울', '스포츠재단', '대표', '촉구']\n",
      "\n",
      "cluster # 239 keywords (from 135 news)\n",
      " > ['뉴시스', '제보', '전주', '영상', '협상', '화성', '수원', '파업', '기록']\n",
      "\n",
      "cluster # 4 keywords (from 59 news)\n",
      " > ['정부서울청사', '종로구', '뉴시스', '사진', '기획재정부', '유일', '하반기']\n",
      "\n",
      "cluster # 741 keywords (from 69 news)\n",
      " > ['고객', '혜택', '추가', '해외', '할인', '디지털', '가능', '마케팅', '투자자', '제출', '제공']\n",
      "\n",
      "cluster # 312 keywords (from 103 news)\n",
      " > ['소재', '적용', '스타일', '착용', '브랜드', '사용', '디자인', '컬러', '날씨', '연출', '가능', '가을', '선택', '제품', '블랙', '라인', '효과']\n",
      "\n",
      "cluster # 42 keywords (from 136 news)\n",
      " > ['김선', '뉴시스', '중구', '컬렉션', '서울패션위크', '디자이너', '사진', '동대문디자인플라자']\n",
      "\n",
      "cluster # 36 keywords (from 62 news)\n",
      " > ['문재인', '뉴시스', '더불어민주당', '사진', '제3', '서울', '영상', '사람']\n",
      "\n",
      "cluster # 968 keywords (from 53 news)\n",
      " > ['6시', '경찰', '출동', '총기', '총격', '피의자', '사제', '강북경찰서', '검거', '현장', '경위', '시민', '터널', '전자발찌', '범인', '폭행']\n",
      "\n",
      "cluster # 183 keywords (from 86 news)\n",
      " > ['박주', '선언', '뉴시스', '손학규', '민주당', '정론관', '사진', '복귀', '여의', '서울']\n",
      "\n",
      "cluster # 619 keywords (from 154 news)\n",
      " > ['권리', '국민일보', '반론', '전화', '우리', '여성', '있습니다', '사랑', '보도', '함께']\n",
      "\n",
      "cluster # 525 keywords (from 79 news)\n",
      " > ['영상', '함께', '동영상', '모습', '유튜브', '공개', '기자', '자신', '바다', '영화']\n",
      "\n",
      "cluster # 401 keywords (from 451 news)\n",
      " > ['전문기자', '불법', '금지', '의견', '대한민국', '조치', '디지털타임스', '무단', '자세', '영국', '독특', '뉴스', '종목', '저작권자', '코엑스', '강의', '친구', '문의', '모두', '전반', '선정', '제1', '있습니다', '전국', '한겨레', '음식', '실시', '꿈꾸는', '01', '유지']\n",
      "\n",
      "cluster # 95 keywords (from 52 news)\n",
      " > ['화보', '매력', '진행', '나라', '패션', '드라마', '분위기', '모습']\n",
      "\n",
      "cluster # 57 keywords (from 79 news)\n",
      " > ['머니투데', '기록']\n",
      "\n",
      "cluster # 521 keywords (from 99 news)\n",
      " > ['머니투데', '공시', '집계', '20일']\n",
      "\n",
      "cluster # 270 keywords (from 96 news)\n",
      " > ['사람', '저녁', '이용', '식사', '함께', '예능', '모습', '실패', '시간', '동안', '방송', '하지', '시작', '이들']\n",
      "\n",
      "cluster # 632 keywords (from 55 news)\n",
      " > ['컬렉션', '패션', '디자이너', '컬러', '디자인', '2017', '블랙', '이번', '스타일', '선보']\n",
      "\n",
      "cluster # 118 keywords (from 64 news)\n",
      " > ['고성', '질투']\n",
      "\n",
      "cluster # 715 keywords (from 76 news)\n",
      " > ['게임', '캐릭터', '콘텐츠', '출시', '이벤트', '개발', '모바일', '기사제공', '효과', '진행']\n",
      "\n",
      "cluster # 237 keywords (from 239 news)\n",
      " > ['뉴스1', '노력', '광주', '홍보', '이날', '운영', '충북', '일부', '이번', '제공', '이라고', '21', '선발', '활동', '지난', '정책', '선정', '수상', '전국', '학생들', '경남', '관계자는', '촉구', '20일', '있도록', '방문', '주민들', '대전', '전남', '활용']\n",
      "\n",
      "cluster # 661 keywords (from 72 news)\n",
      " > ['무단', '금지', '재배포', '200', '매일경제']\n",
      "\n",
      "cluster # 642 keywords (from 110 news)\n",
      " > ['인스타그램', '사진', '한편', '함께', '자신', '공개', '글과', '게재', '팬들']\n",
      "\n",
      "cluster # 91 keywords (from 69 news)\n",
      " > ['루이', '쇼핑왕', '패션']\n",
      "\n",
      "cluster # 909 keywords (from 61 news)\n",
      " > ['불독', '키미', '기념', '싱글', '세이', '생각', '데뷔', '이날', '콘셉트', '라고', '사진', '형은']\n",
      "\n",
      "cluster # 753 keywords (from 94 news)\n",
      " > ['서울경제', '12월', '배우']\n",
      "\n",
      "cluster # 847 keywords (from 52 news)\n",
      " > ['뉴스1스타', '오후', '디자이너', '컬렉션', '헤라', '서울', '포즈']\n",
      "\n",
      "cluster # 927 keywords (from 53 news)\n",
      " > ['헤럴드경제']\n",
      "\n",
      "cluster # 765 keywords (from 90 news)\n",
      " > ['이데일리', '제공', '출시', '20일', '기부', '김용', '체험', '행사', '기자', '정원']\n",
      "\n",
      "cluster # 26 keywords (from 57 news)\n",
      " > ['이데일리', '현재']\n",
      "\n",
      "cluster # 965 keywords (from 53 news)\n",
      " > ['이데일리', '공시', '02', '20일', '79', '계약', '기자']\n",
      "\n",
      "cluster # 236 keywords (from 55 news)\n",
      " > ['사랑', '함께', '스타', '영화']\n",
      "\n",
      "cluster # 156 keywords (from 51 news)\n",
      " > ['세계일보', '중구', '포토타임', '서울패션위크', '미디어', '세상']\n",
      "\n",
      "cluster # 138 keywords (from 52 news)\n",
      " > ['기관', '최근', '현재', '외국인', '거래']\n",
      "\n",
      "cluster # 254 keywords (from 80 news)\n",
      " > ['엔터온뉴스', '을지', '취하고', '전자신문', '서울패션위크', '컬렉션', '포즈']\n",
      "\n",
      "cluster # 107 keywords (from 130 news)\n",
      " > ['동대문', '스포츠조선', '동대문디자인플라자', '서울패션위크']\n",
      "\n",
      "cluster # 117 keywords (from 147 news)\n",
      " > ['08', '2016']\n",
      "\n",
      "cluster # 139 keywords (from 74 news)\n",
      " > ['헤라서울패션위크', '김창', '스타뉴스', '컬렉션']\n",
      "\n",
      "cluster # 68 keywords (from 104 news)\n",
      " > ['패션위크', '서울', '을지', '중구']\n",
      "\n",
      "cluster # 322 keywords (from 83 news)\n",
      " > ['마이데일리', '패션', '기사', '사진']\n",
      "\n",
      "cluster # 15 keywords (from 52 news)\n",
      " > ['마이데일리', '사랑', '영화']\n",
      "\n",
      "cluster # 13 keywords (from 280 news)\n",
      " > ['유진', '디자이너', '진행', '브랜드', '마이데일리']\n",
      "\n",
      "cluster # 722 keywords (from 112 news)\n",
      " > ['포즈', '디자이너', '문수', '김홍범', '다양', '트레이드', '헤라', '취하고', '패션축제', '콜렉션', '조망', '서울패션위크']\n",
      "\n",
      "cluster # 5 keywords (from 106 news)\n",
      " > ['박세', '일간스포츠']\n",
      "\n",
      "cluster # 191 keywords (from 216 news)\n",
      " > ['아시아경제', '보는', '경제', '세계', '전남', '기자', '있도록', '지원', '국내', '사용', '이번']\n",
      "\n",
      "cluster # 45 keywords (from 71 news)\n",
      " > ['엑스포츠뉴스', '을지', '오후']\n",
      "\n",
      "cluster # 11 keywords (from 156 news)\n",
      " > ['방지', '동아닷컴', '스포츠동아', '서울패션위크', '동대문디자인플라자', '헤라']\n",
      "\n",
      "cluster # 917 keywords (from 93 news)\n",
      " > ['뉴스1', '서울', '마감', '공공기관', '금융', '고객', '총격', '미래창조과학부']\n",
      "\n",
      "cluster # 80 keywords (from 86 news)\n",
      " > ['추연', '제공', '뉴스1', '장관', '2016', '기획재정부', '일본']\n",
      "\n",
      "cluster # 447 keywords (from 66 news)\n",
      " > ['뉴스1', '33', '20일', '부산', '함께', '경남', '119', '울산', '현지시간']\n",
      "\n",
      "cluster # 126 keywords (from 53 news)\n",
      " > ['손학규', '상임고문', '뉴스1', '기자회견', '서울', '정론관', '취재진', '복귀', '국회']\n",
      "\n",
      "cluster # 59 keywords (from 74 news)\n",
      " > ['버튼', '크기', '감상', '09', '뮤직비디오', '27', '이미지', '눈물', '방탄소년단']\n"
     ]
    }
   ],
   "source": [
    "def print_keywords(interested_clusters, x, int2vocab, clusters_to_rows, C=1, topn=30):\n",
    "    \n",
    "    for cluster_id in interested_clusters:\n",
    "        interested_docs = clusters_to_rows[cluster_id]\n",
    "        print('\\ncluster # %d keywords (from %d news)' % (cluster_id, len(interested_docs)))\n",
    "        y = [1 if i in interested_docs else -1 for i in range(x_2016_1020.shape[0])]\n",
    "        \n",
    "        logistic_l1 = LogisticRegression(penalty='l1', C=C)\n",
    "        logistic_l1.fit(x, y)\n",
    "\n",
    "        keywords = sorted(enumerate(logistic_l1.coef_[0]), key=lambda x:x[1], reverse=True)[:topn]\n",
    "        keywords = [int2vocab[word] for word, score in keywords if score > 0]\n",
    "        print(' > %s' % (keywords))\n",
    "        \n",
    "interested_clusters = [i for i, labels in clusters_to_rows.items() if 50 < len(labels) < 500]\n",
    "print_keywords(interested_clusters, x_2016_1020, int2vocab, clusters_to_rows, C=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (python3_0901)",
   "language": "python",
   "name": "python3_0901"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:21:27.988695Z",
     "start_time": "2018-03-13T09:21:27.753674Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus_fname = '/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/news/2016-10-24_article_all_normed.txt'\n",
    "mm_fname = '/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/models/2016-10-24_article_all_normed_corpus.mtx'\n",
    "mm_vocab = '/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/models/2016-10-24_article_all_normed_corpus.vocab'\n",
    "dictionary_fname = '/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/models/2016-10-24_article_all_normed_corpus.dictionary'\n",
    "ldamodel_fname = '/home/paulkim/workspace/python/Korean_NLP/data/corpus_10days/models/2016-10-24_article_all_normed_corpus_lda.pkl'\n",
    "\n",
    "import pickle\n",
    "from corpus import Corpus\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.io import mmwrite, mmread\n",
    "\n",
    "PREPROCESSING = True # False\n",
    "TRAIN_LDA = True # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:22:18.461255Z",
     "start_time": "2018-03-13T09:22:09.942195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num docs =  26368\n",
      "(26368, 4760)\n"
     ]
    }
   ],
   "source": [
    "if PREPROCESSING:\n",
    "    corpus = Corpus(corpus_fname, iter_sent=False)\n",
    "    print('num docs = ', len(corpus))\n",
    "    \n",
    "    with open('tmp/extracted_noun_dict.pkl', 'rb') as f:\n",
    "        noun_dict = pickle.load(f)\n",
    "        \n",
    "    def custom_tokenize(doc):\n",
    "        def parse_noun(token):\n",
    "            for e in reversed(range(1, len(token) + 1)):\n",
    "                subword = token[:e]\n",
    "                if subword in noun_dict:\n",
    "                    return subword\n",
    "            return ''\n",
    "        \n",
    "        nouns = [parse_noun(token) for token in doc.split()]\n",
    "        nouns = [word for word in nouns if word]\n",
    "        return nouns\n",
    "    \n",
    "    \n",
    "    vectorizer = CountVectorizer(tokenizer=custom_tokenize)\n",
    "    x = vectorizer.fit_transform(corpus)\n",
    "    mmwrite(mm_fname, x)\n",
    "    with open(mm_vocab, 'w', encoding='utf-8') as f:\n",
    "        for word, _ in sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1]):\n",
    "            f.write('%s\\n'%word)\n",
    "            \n",
    "    print(x.shape)\n",
    "    \n",
    "else:\n",
    "    x = mmread(mm_fname)\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim tutorial\n",
    "\n",
    "### [Corpus Formats][corpus_format]\n",
    "\n",
    "Gensim의 LDA가 이용하는 학습데이터의 형식은 list of list of tuple이며, tuple은 (term_index, frequency) 임\n",
    "\n",
    "\n",
    "    >>> from gensim import corpora\n",
    "    \n",
    "    >>> # create a toy corpus of 2 documents, as a plain Python list\n",
    "    >>> corpus = [[(1, 0.5)], []]  # make one document empty, for the heck of it\n",
    "    >>>\n",
    "    >>> corpora.MmCorpus.serialize('/tmp/corpus.mm', corpus)\n",
    "    \n",
    "혹은 만약 sparse matrix를 만든 다음, mmwrite를 이용하여 저장하였다면 아래처럼 term frequency matrix 파일을 불러들일수도 있습니다. \n",
    "\n",
    "\n",
    "    >>> scipy_sparse_matrix = mmread(mm_fname)    \n",
    "    >>> corpus = gensim.matutils.Sparse2Corpus(scipy_sparse_matrix)\n",
    "\n",
    "그런데, gensim은 sparse matrix를 (doc, term) matrix가 아니라 (term, doc) matrix라고 가정합니다. 그래서 sparse matrix를 transpose() 해야 합니다. \n",
    "\n",
    "    >>> corpus = gensim.matutils.Sparse2Corpus(scipy_sparse_matrix.transpose())\n",
    "\n",
    "\n",
    "\n",
    "[corpus_format]: https://radimrehurek.com/gensim/tut1.html#corpus-formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:22:35.974325Z",
     "start_time": "2018-03-13T09:22:35.767641Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# sparse matrix를 (doc, term) matrix가 아닌 (term, doc) matrix로 변경하기 위해 transpose\n",
    "corpus = gensim.matutils.Sparse2Corpus(x.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:22:40.104497Z",
     "start_time": "2018-03-13T09:22:40.099864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#doc= 0: [] ...\n",
      "\n",
      "#doc= 1: [(4592, 1), (818, 2), (697, 1), (3202, 1), (4678, 1), (530, 3), (1188, 1), (1605, 1), (3152, 1), (2104, 1)] ...\n",
      "\n",
      "#doc= 2: [(2552, 1), (4671, 1), (1174, 1), (3678, 1), (4615, 2), (447, 1), (4740, 1), (1871, 1), (3131, 1), (3679, 1)] ...\n",
      "\n",
      "#doc= 3: [(3868, 1), (3513, 1), (2701, 1), (2117, 1), (756, 1), (2858, 1), (1890, 1), (2530, 1), (2189, 1), (469, 1)] ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(corpus):\n",
    "    if i > 3: break\n",
    "    print('#doc= %d:'%i, doc[:10], '...\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:22:41.284648Z",
     "start_time": "2018-03-13T09:22:41.275450Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26368"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary Formats\n",
    "\n",
    "Gensim은 우리가 이용하는 Vectorizer.vocabulary_와 같은 역할로 Dictionary라는 class를 이용하고 있음. 이는 topic modeling의 결과를 0, 1, 2와 같은 term index가 아닌 단어로 보여주는 역할을 함. Format이 조금 다르기 때문에 Vectorizer를 이용하여 term frequency matrix를 만든 뒤, 그 결과를 이용해 Dictionary format의 파일을 만들어낼 것임\n",
    "\n",
    "아래와 같은 text라는 list of list of str 형식의 docs가 있을 때, 이를 이용하여 dictionary를 만들고 저장하는 방법은 아래와 같음\n",
    "\n",
    "    texts = [['human', 'interface', 'computer'],\n",
    "    ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
    "    ['system', 'user', 'interface', 'system'],\n",
    "    ['user', 'response', 'time'],\n",
    "    ['tree'],\n",
    "    ['graph', 'trees'],\n",
    "    ['graph', 'minors', 'trees'],\n",
    "    ['graph', 'minors', 'survey']]\n",
    "    \n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    dictionary.save_as_text('dictionary')\n",
    "    \n",
    "dictionary라는 파일에 Dictionary의 정보가 저장됨. 저장된 내용은 아래와 같음\n",
    "\n",
    "    9\n",
    "    2\tcomputer\t2\n",
    "    8\teps\t2\n",
    "    10\tgraph\t3\n",
    "    0\thuman\t2\n",
    "    1\tinterface\t2\n",
    "    11\tminors\t2\n",
    "    6\tresponse\t2\n",
    "    3\tsurvey\t2\n",
    "    5\tsystem\t3\n",
    "    7\ttime\t2\n",
    "    9\ttrees\t3\n",
    "    4\tuser\t3\n",
    "    \n",
    "맨 첫줄에 num docs가 들어있고, 그 다음줄부터 (term index, term, document frequency)가 tab구분이 되어 저장됨. Vectorizer.vocabulary_와 x를 이용하면 동일한 형식의 dictionary를 만들 수 있음\n",
    "\n",
    "Sparse Matrix는 아래처럼 세 개의 list가 평행하게 움직임\n",
    "\n",
    "    rows = [1, 1, 2, 5, ...]\n",
    "    cols = [13, 734, 0, 4, ...]\n",
    "    data = [1, 3, 1, 9, ...]\n",
    "    \n",
    "이는 아래의 의미임\n",
    "\n",
    "    (1, 13) = 1\n",
    "    (1, 734) = 3\n",
    "    \n",
    "data는 term frequency인데, 이 값이 1보다 크면 모두 1로 바꾸면 됨. 이 부분이 아래의 코드임\n",
    "\n",
    "    row, col = x.nonzero()\n",
    "    data = [1] * len(row)\n",
    "    x_boolean = csr_matrix((data, (row, col)))\n",
    "    \n",
    "그 다음은 column을 중심으로 row를 합치면 단어의 document frequency가 구해짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사전을 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:22:57.028157Z",
     "start_time": "2018-03-13T09:22:56.833176Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "row, col = x.nonzero()\n",
    "data = [1] * len(row)\n",
    "x_boolean = csr_matrix((data, (row, col)))\n",
    "df = x_boolean.sum(axis = 0)\n",
    "\n",
    "n_doc = x.shape[0]\n",
    "word2index = vectorizer.vocabulary_\n",
    "df = df.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:22:57.842600Z",
     "start_time": "2018-03-13T09:22:57.839510Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4760"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:22:58.620437Z",
     "start_time": "2018-03-13T09:22:58.616677Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26368"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:24:55.640977Z",
     "start_time": "2018-03-13T09:24:55.632814Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(dictionary_fname, 'w', encoding='utf-8') as f:\n",
    "    f.write('%d\\n'%n_doc)\n",
    "    for word, idx in word2index.items():\n",
    "        f.write('%d\\t%s\\t%d\\n'%(idx, word, df[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:25:06.582599Z",
     "start_time": "2018-03-13T09:25:06.579535Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `LdaModel` not found.\n"
     ]
    }
   ],
   "source": [
    "LdaModel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:25:10.665524Z",
     "start_time": "2018-03-13T09:25:10.651493Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary.load_from_text(dictionary_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:25:11.446579Z",
     "start_time": "2018-03-13T09:25:11.443083Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3378, 1), (3588, 1), (3614, 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.doc2bow(['정부', '정책', '자유'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 gensim LDA format으로 맞추는 일이 끝남. dictionary와 corpus를 이용하여 LDA를 학습함\n",
    "\n",
    "num_topics는 토픽의 갯수를 정하는 부분임. id2word를 입력하지 않으면 단어가 term index로 출력됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:25:14.676555Z",
     "start_time": "2018-03-13T09:25:14.673935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `LdaModel` not found.\n"
     ]
    }
   ],
   "source": [
    "LdaModel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:25:15.500023Z",
     "start_time": "2018-03-13T09:25:15.496270Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26368"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:27:07.943956Z",
     "start_time": "2018-03-13T09:25:44.114414Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "import pickle\n",
    "\n",
    "if TRAIN_LDA:\n",
    "    ldamodel = LdaModel(corpus=corpus, num_topics=50, id2word=dictionary)\n",
    "    \n",
    "    with open(ldamodel_fname, 'wb') as f:\n",
    "        pickle.dump(ldamodel, f)\n",
    "        \n",
    "else:\n",
    "    with open(ldamodel_fname, 'rb') as f:\n",
    "        ldamodel = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print_topic은 특정topic에 대하여 설명력이 좋은 (topic probability가 높은) topn개의 단어를 prob.와 함께 출력해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:27:43.440645Z",
     "start_time": "2018-03-13T09:27:43.434745Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.028*\"웃음\" + 0.023*\"있습니다\" + 0.019*\"우리\" + 0.014*\"정부\" + 0.013*\"지원\"'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topic(10, topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_topic_terms(topic_id)를 하면 term index가 출력되기 때문에 print_topic()의 결과에서 단어만 선택하는 sparse_topic_words()함수를 만들어 50개의 토픽에 대하여 각각 대표단어를 뽑아낼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:28:58.764215Z",
     "start_time": "2018-03-13T09:28:58.757737Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2863, 0.059436067315897227),\n",
       " (355, 0.031683337759101725),\n",
       " (1005, 0.027690090448755079),\n",
       " (2775, 0.021355080690100687),\n",
       " (1117, 0.019250735874195023),\n",
       " (2430, 0.017368964752001712),\n",
       " (405, 0.015494830637440482),\n",
       " (3400, 0.014352134978649091),\n",
       " (2359, 0.011918621148862078),\n",
       " (4700, 0.011850607329534475)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.get_topic_terms(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:30:55.626330Z",
     "start_time": "2018-03-13T09:30:55.623682Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_topic_words(topic_str):\n",
    "    return [col.split('*\"')[1][:-1] for col in topic_str.split(' + ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:32:36.227736Z",
     "start_time": "2018-03-13T09:32:36.194275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#topic= 0: ['개헌', '대통령', '지금', '국민', '문제', '경제', '정치', '상황', '생각', '논의']\n",
      "#topic= 1: ['영화', '감독', '남자', '여자', '닥터', '스트레인지', '개봉', '작품', '수상', '활약']\n",
      "#topic= 2: ['요리', '고양이', '코미디', '화면', '관객', '미스', '이동', '24', '대화', '영화']\n",
      "#topic= 3: ['저작권자', '전남', '경남', '광주', '24', '선정', '한강', '동상', '헤럴드경제', '순천']\n",
      "#topic= 4: ['뉴스1', '보험', '2014년', '배포', '회의', '당시', '할인', '결정', '기억', '확인']\n",
      "\n",
      "#topic= 5: ['르노삼성', '검색', '24', '웹툰', '금괴', '인천공항', '출국', '인천국제공항', '사이트', '뉴스']\n",
      "#topic= 6: ['기업', '회장', '사업', '삼성', '한국', '지원', '창업', '경영', '임금', '케미']\n",
      "#topic= 7: ['북한', '미국', '정부', '외교', '대북', '대화', '러시아', '문제', '압박', '북핵']\n",
      "#topic= 8: ['시장', '중국', '미국', '인체', '인천', '공장', '국내', '제품', '생산', '글로벌']\n",
      "#topic= 9: ['한국', '작품', '세계', '사로', '예술', '작가', '문화', '인간', '도시', '자리']\n",
      "\n",
      "#topic= 10: ['웃음', '있습니다', '우리', '정부', '지원', '경제', '확대', '국민', '예산', '합니다']\n",
      "#topic= 11: ['참석', '24', '오후', '대표', '북한', '금융', '회고록', '장관', '문재인', '헤어질까']\n",
      "#topic= 12: ['24', '한남동', '2016', '세번째', '경찰', '모모', '집행', '티켓', '오후', '영장']\n",
      "#topic= 13: ['중앙', '중국', '것으', '한의사', '정부', '국방부', '이번', '박정희', '회의', '이후']\n",
      "#topic= 14: ['방송', '사진', '배우', '공개', '모습', '드라마', '데뷔', '사랑', '출연', '24']\n",
      "\n",
      "#topic= 15: ['아파트', '건물', '안무', '입주', '연기', '배용준', '발레', '대우', '임대', '첫사랑']\n",
      "#topic= 16: ['네이버', '제주', '제주도', '개발', '기술', '곽동', '환경', '주차', '24', '음성']\n",
      "#topic= 17: ['여성', '가격', '것으', '루이', '인도네시아', '지난', '평균', '키스', '남성', '스크린']\n",
      "#topic= 18: ['사업', '지원', '추진', '서울시', '행복', '지역', '문화', '신설', '계획', '시설']\n",
      "#topic= 19: ['2016', '24', '제보', '23', '영상', '뉴스', '사진', '빅스', '최진', '20주년']\n",
      "\n",
      "#topic= 20: ['학교', '교육', '학생들', '대학', '아내', '라디오', '쇼핑', '이목', '등록금', '그녀']\n",
      "#topic= 21: ['의장', '골프', '헤드', '문화재', '서현진', '유연석', '병원', '단풍', '낭만', '가을']\n",
      "#topic= 22: ['게임', '제공', '체험', '진행', '여행', '온라인', '우수', '모바일', '이벤트', '이번']\n",
      "#topic= 23: ['일본', '청소년', '무릎', '도쿄', '농업', '아프리카', '영국', '협력', '24', '유일']\n",
      "#topic= 24: ['경우', '것으', '환자', '사용', '정부', '치료', '결과', '하지', '처방', '예산']\n",
      "\n",
      "#topic= 25: ['단지', '피부', '분양', '지역', '중국', '예정', '주거', '지하', '위치', '수술']\n",
      "#topic= 26: ['기사', '스포츠', '박수', '인턴', '24', '시선', '공정위', '눈빛', '저축', '명예']\n",
      "#topic= 27: ['트와이스', '걸그룹', '24', '쇼케이스', '미니앨범', '신곡', '블루스퀘어', '발매', '앨범', '번째']\n",
      "#topic= 28: ['장애인', '고객', '신청', '가입', '고용', '매거진', '24', '사기꾼', '뮤직', '이상']\n",
      "#topic= 29: ['건강', '경우', '이별', '치매', '사과', '위자료', '역도', '논란', '연구', '우리']\n",
      "\n",
      "#topic= 30: ['경찰', '것으', '사고', '발생', '조사', '대구', '치료', '병원', '오후', '아들']\n",
      "#topic= 31: ['갤럭시', '클린턴', '트럼프', '교환', '삼성전자', '갤럭시노트7', '후보', '미국', '프로그램', '노트7']\n",
      "#topic= 32: ['한국일보', '현대', '법원', '소송', '변호사', '사실', '남편', '사건', '판결', '재판']\n",
      "#topic= 33: ['뉴스1', '결별', '대통령', '구르미', '진영', '필리핀', '세부', '마약', '한국경제', '디자이너']\n",
      "#topic= 34: ['지진', '경기도', '지역', '한글', '수원', '발생', '경기', '대전', '오전', '경주']\n",
      "\n",
      "#topic= 35: ['사진', '음악', '공연', '공개', '인스타그램', '앨범', '활동', '지난', '노래', '가수']\n",
      "#topic= 36: ['사람', '자신', '라고', '생각', '방송', '모습', '하지', '이야기', '때문', '배우']\n",
      "#topic= 37: ['투자', '부동산', '상승', '거래', '주가', '금융', '기록', '대출', '매수', '은행']\n",
      "#topic= 38: ['독도', '총장', '수록', '25일', '소감', '대학', '뉴스1', '배우들', '24', '성형']\n",
      "#topic= 39: ['00', '부산', '앵커', '2016', '일간스포츠', '24', '000원', '비정상', '블락비', '23']\n",
      "\n",
      "#topic= 40: ['서비스', '이용', '기술', '사용', '제품', '정보', '제공', '활용', '이준기', '가능']\n",
      "#topic= 41: ['서울신문', '경제', '방탄', '지지', '전자신문', '이데일리', '지지율', '강태', '종합', '대통령']\n",
      "#topic= 42: ['브랜드', '패션', '판매', '제품', '디자인', '상품', '선보', '다양', '모델', '구매']\n",
      "#topic= 43: ['개헌', '대통령', '국회', '논의', '박근혜', '대표', '국민', '제안', '임기', '청와대']\n",
      "#topic= 44: ['금리', '것으', '기온', '이라크', '정부', '터키', '날씨', '평균', '인상', '아침']\n",
      "\n",
      "#topic= 45: ['뉴스1', '24', '국회', '예산안', '대통령', '시정연설', '여의도', '박근혜', '예산', '관련']\n",
      "#topic= 46: ['국내', '것으', '취업', '올해', '증가', '해외', '지난해', '연구', '수산', '기업']\n",
      "#topic= 47: ['롯데', '사진', '김영철', '의상', '할로윈', '파티', '3000', '훈훈', '호텔롯데', '24']\n",
      "#topic= 48: ['아시아경제', '행사', '함께', '지역', '진행', '다양', '참여', '문화', '축제', '체험']\n",
      "#topic= 49: ['스포츠', '검찰', '의혹', '최씨', '수사', '것으', '재단', '최순실', '독일', '관련']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    print('#topic= %d:'%i, parse_topic_words(ldamodel.print_topic(i, topn=10)))\n",
    "    if i % 5 == 4: print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim에서 단어의 topic vector를 직접적으로 찾아주는 함수가 구현되어 있지 않음. 하지만 LdaModel[bow_model]을 넣으면 bow_model에 대한 topic vector를 출력해줌. bow_model에 단어 한 개를 넣으면 해당 단어의 topic vector를 알 수 있음\n",
    "\n",
    "bow_model은 [(term id, weight), (term id, weight), ...] 형식임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (python3_0901)",
   "language": "python",
   "name": "python3_0901"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

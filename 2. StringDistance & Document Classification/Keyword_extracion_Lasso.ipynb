{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword_extraction_with_Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "small naver news로부터 명사로 이뤄진 term frquency matrix를 만든 뒤, L1 regularization logistic regression을 이용하여 키워드를 추출한다\n",
    "\n",
    "명사 추출을 위해 soynlp의 LRNounExtractor를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T12:01:13.006782Z",
     "start_time": "2018-03-10T12:01:12.940927Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8818"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "\n",
    "corpus_path = \"/home/paulkim/workspace/python/Korean_NLP/data/small_naver_news/processed/corpus.txt\"\n",
    "corpus = DoublespaceLineCorpus(corpus_path, iter_sent=True)\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "noun score threshold는 0.2 이상, min count는 10 이상인 명사만을 선택하여 custom_tokenizer를 만듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T12:01:14.341529Z",
     "start_time": "2018-03-10T12:01:13.674631Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used default noun predictor; Sejong corpus predictor\n",
      "used noun_predictor_sejong\n",
      "2398 r features was loaded\n",
      "scanning completed\n",
      "(L,R) has (6862, 3655) tokens\n",
      "building lr-graph completed"
     ]
    }
   ],
   "source": [
    "from soynlp.noun import LRNounExtractor\n",
    "\n",
    "noun_extrator = LRNounExtractor()\n",
    "nouns = noun_extrator.train_extract(corpus, min_count=10, minimum_noun_score=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T12:01:14.836697Z",
     "start_time": "2018-03-10T12:01:14.806188Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2046"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parse_noun은 주어진 token에 대하여 어절의 왼쪽에 명사가 존재할 경우 이를 잘라내주는 함수임. 만약 '뉴스기자가'라는 어절에 대해 '뉴스'와 '뉴그시자' 두 명사가 모두 존재한다면 길이가 더 긴 '뉴스기자'를 return함. 이를 위하여 reversed(range)를 이용하여 길이의 역순으로 명사를 선택했음\n",
    "\n",
    "문서가 주어지면 추출된 명사만을 출력하는 custom_tokenize를 만듬. 이를 이용하면 tokenize, part of speech tagging, 이후 명사만 추출하는 과정을 거친 것과 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T12:01:15.636643Z",
     "start_time": "2018-03-10T12:01:15.596869Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['국정', '관련', '뉴스']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_tokenize(doc):    \n",
    "    def parse_noun(token):\n",
    "        for e in reversed(range(1, len(token)+1)):\n",
    "            subword = token[:e]\n",
    "            # soynlp.noun.LRNounExtractor객체로 만들어진 사전\n",
    "            if subword in nouns:\n",
    "                return subword\n",
    "        return ''\n",
    "    \n",
    "    words = [parse_noun(token) for token in doc.split()]\n",
    "    words = [word for word in words if word]\n",
    "    return words\n",
    "\n",
    "custom_tokenize('국정에 관련된 뉴스입니다')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency matrix 만들기\n",
    "sklearn의 CountVectorizer를 이용하여 term frequency matrix를 만든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T12:01:16.807321Z",
     "start_time": "2018-03-10T12:01:16.478845Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1355x2028 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 44172 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=custom_tokenize)\n",
    "\n",
    "corpus.iter_sent = False\n",
    "x = vectorizer.fit_transform(corpus)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T12:01:17.263586Z",
     "start_time": "2018-03-10T12:01:17.249045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"앞', '\"앞으', '\"오늘', '\"올', '\"우리']\n"
     ]
    }
   ],
   "source": [
    "index2word = sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1])\n",
    "index2word = [word for word, i in index2word]\n",
    "print(index2word[15:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T12:01:17.661629Z",
     "start_time": "2018-03-10T12:01:17.654684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['130주년', '13일', '14', '14일', '15일']\n"
     ]
    }
   ],
   "source": [
    "print(index2word[50:55])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification을 위한 positive / negative set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "국회라는 단어가 들어간 문서와 그렇지 않은 문서로 1355개의 문서의 class를 나눈 뒤, 이를 구분할 수 있는 classifier를 학습하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T12:01:18.967793Z",
     "start_time": "2018-03-10T12:01:18.960298Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "국회 id = 384\n",
      "num of doc (국회) = 208\n"
     ]
    }
   ],
   "source": [
    "aspect_word = '국회'\n",
    "aspect_id = vectorizer.vocabulary_.get(aspect_word, -1)\n",
    "\n",
    "print(\"%s id = %d\"%(aspect_word, aspect_id))\n",
    "print('num of doc (%s) = %d'%(aspect_word, len(x[:, aspect_id].nonzero()[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sparse matrix는 nonzero()라는 함수가 있으며, matrix에서 값이 0이 아닌 위치를 (row id list, column id list)로 나타내줌. aspect_id에 해당하는 컬럼을 떼어냈기 때문에 submatrix의 모양이 (1355, 1)이 되었음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T12:01:19.800915Z",
     "start_time": "2018-03-10T12:01:19.791182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1355, 1)\n",
      "(array([   2,   16,   18,   20,   27,   33,   39,   52,   62,   76,   85,\n",
      "         89,  102,  105,  111,  112,  129,  137,  151,  163,  176,  215,\n",
      "        218,  220,  223,  224,  226,  240,  245,  248,  254,  255,  292,\n",
      "        298,  299,  304,  342,  343,  356,  357,  359,  365,  372,  384,\n",
      "        392,  405,  409,  425,  426,  428,  431,  432,  438,  443,  457,\n",
      "        463,  464,  467,  470,  499,  500,  503,  507,  515,  516,  520,\n",
      "        521,  526,  543,  548,  555,  557,  564,  566,  568,  571,  576,\n",
      "        581,  597,  598,  606,  607,  620,  628,  635,  642,  652,  663,\n",
      "        667,  672,  679,  687,  695,  697,  701,  707,  710,  712,  715,\n",
      "        716,  733,  735,  736,  738,  744,  750,  751,  754,  755,  759,\n",
      "        769,  773,  776,  780,  781,  802,  817,  825,  827,  831,  857,\n",
      "        859,  863,  870,  879,  881,  890,  894,  896,  911,  916,  931,\n",
      "        934,  948,  955,  958,  959,  969,  990,  998, 1002, 1006, 1008,\n",
      "       1009, 1011, 1017, 1031, 1033, 1037, 1065, 1066, 1067, 1071, 1073,\n",
      "       1080, 1095, 1100, 1101, 1102, 1105, 1112, 1113, 1114, 1116, 1117,\n",
      "       1118, 1141, 1144, 1148, 1149, 1151, 1152, 1159, 1168, 1172, 1173,\n",
      "       1175, 1176, 1183, 1207, 1211, 1216, 1219, 1221, 1223, 1224, 1229,\n",
      "       1239, 1247, 1253, 1254, 1273, 1284, 1290, 1300, 1302, 1305, 1308,\n",
      "       1309, 1310, 1315, 1318, 1320, 1321, 1331, 1342, 1344, 1348], dtype=int32),\n",
      " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "print(x[:, aspect_id].shape)\n",
    "from pprint import pprint\n",
    "pprint(x[:, aspect_id].nonzero())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'국회'란 단어가 들어간 문서들의 리스트를 sparse matrix에서 가져왔음. 이를 이용하여 각 문서에 '국회'라는 단어가 들어있으면 1, 아니면 -1인 label list y를 만든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T12:01:20.678281Z",
     "start_time": "2018-03-10T12:01:20.673035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape = (1355, 2028), len(y) = 1355, num_pos = 208\n"
     ]
    }
   ],
   "source": [
    "pos_idx = set(x[:, aspect_id].nonzero()[0]) # in함수는 list보다 set이 빠름\n",
    "y = [1 if i in pos_idx else -1 for i in range(x.shape[0])]\n",
    "\n",
    "print('x shape = %s, len(y) = %d, num_pos = %d'%(str(x.shape), len(y), len(pos_idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 (Lasso)를 이용한 keyword extraction\n",
    "L1 regularization을 이용함. 이는 '국회'가 들어간 문서와 아닌 문서를 구분하기 위한 최소한의 features(=nouns)를 선택하는 효과가 있음\n",
    "\n",
    "여기서 정의하는 키워드의 기준은, 하난의 class set과 다른 class set을 구분할 수 있는 최소한의 단어집합임. 키워드를 추출하기 위함이기 때문에 이번에는 x_train 대신, '국회'라는 단어가 포함된 term frequency matrix인 x를 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T12:01:21.540788Z",
     "start_time": "2018-03-10T12:01:21.501970Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_l1 = LogisticRegression(penalty='l1')\n",
    "logistic_l1.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1의 regularization cost에 따른 키워드 추출의 경향을 확인함. cost별로 beta의 크기가 큰 top 50개의 키워드들을 top50 dict에 저장해둠\n",
    "\n",
    "costs는 1/C의 값임. C = 500이면 lambda = 1/500이므로, regularization이 매우 작게 됨을 의미함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T12:01:22.668773Z",
     "start_time": "2018-03-10T12:01:22.626266Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with cost = 500.0\n",
      "done with cost = 200.0\n",
      "done with cost = 100.0\n",
      "done with cost = 50.0\n"
     ]
    }
   ],
   "source": [
    "costs = [500, 200, 100, 50]\n",
    "top50 = {}\n",
    "\n",
    "for cost in costs:\n",
    "    logistic_l1 = LogisticRegression(penalty='l1', C=cost)\n",
    "    logistic_l1.fit(x, y)\n",
    "    \n",
    "    coef_l1 = logistic_l1.coef_.reshape(-1)\n",
    "    beta_l1 = [(index2word[i], coef) for i, coef in enumerate(coef_l1)]\n",
    "    beta_l1 = sorted(beta_l1, key=lambda x:x[1], reverse=True)\n",
    "    top50[cost] = beta_l1[:50]\n",
    "    print('done with cost = %.1f'%cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization이 강하게 되면서, 국회라는 단어가 들어간 문서집합을 설명하기 위해 선택한 단어의 갯수가 줄어듬. 이를 국회라는 문서가 들어간 문서집합의 키워드로 추출할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T12:01:23.553818Z",
     "start_time": "2018-03-10T12:01:23.543737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        국회\t        국회\t        국회\t        국회\n",
      "    최고위원회의\t        배재\t        배재\t     서울구치소\n",
      "   대표(오른쪽)\t    최고위원회의\t      당대표실\t        의원\n",
      "        배재\t   대표(오른쪽)\t    최고위원회의\t       최순실\n",
      "        고발\t      진상규명\t      국정조사\t        의혹\n",
      "      국조특위\t      국조특위\t      진상규명\t       청문회\n",
      "      '최순실\t        고발\t      원내대표\t        자택\n",
      "      의원회관\t      당대표실\t        의원\t        실장\n",
      "      원내대표\t        사표\t       최순실\t        총장\n",
      " 국정조사특별위원회\t        확보\t        의혹\t       문체부\n",
      "      국정농단\t        오전\t        자택\t      원내대표\n",
      "        사표\t        의원\t        사표\t        대표\n",
      "       구치소\t     서울구치소\t       의원들\t          \n",
      "      당대표실\t        현장\t   대표(오른쪽)\t          \n",
      "      진상규명\t       최순실\t        현장\t          \n",
      "        이혼\t        탈당\t     서울구치소\t          \n",
      "        공모\t        총장\t        합병\t          \n",
      "        귀국\t      압수수색\t        실장\t          \n",
      "      새누리당\t       공무원\t        총장\t          \n",
      "        \"내\t        실장\t        오전\t          \n",
      "        탈당\t      원내대표\t      국정농단\t          \n",
      "        현장\t       청문회\t        확보\t          \n",
      "       최순실\t       대통령\t      새누리당\t          \n",
      "        과거\t        합병\t      압수수색\t          \n",
      "        확보\t       청와대\t        의결\t          \n",
      "        의사\t        대표\t       민간인\t          \n",
      "     현장청문회\t      비서실장\t          \t          \n",
      "        의원\t        의혹\t          \t          \n",
      "        '국\t        수사\t          \t          \n",
      "        발표\t       전국위\t          \t          \n",
      "      국민의당\t        의결\t          \t          \n",
      "        전망\t      국정농단\t          \t          \n",
      "        사실\t      권한대행\t          \t          \n",
      "        증인\t          \t          \t          \n",
      "       대해서\t          \t          \t          \n",
      "      국정조사\t          \t          \t          \n",
      "     서울구치소\t          \t          \t          \n",
      "        경기\t          \t          \t          \n",
      "        이날\t          \t          \t          \n",
      "        의결\t          \t          \t          \n",
      "        의혹\t          \t          \t          \n",
      "        최씨\t          \t          \t          \n",
      "        수리\t          \t          \t          \n",
      "       정의당\t          \t          \t          \n",
      "        부문\t          \t          \t          \n",
      "        대표\t          \t          \t          \n",
      "      비서실장\t          \t          \t          \n",
      "        제출\t          \t          \t          \n",
      "        오전\t          \t          \t          \n",
      "        자택\t          \t          \t          \n"
     ]
    }
   ],
   "source": [
    "for top in range(50):\n",
    "    message = '\\t'.join(['%10s'%(top50[cost][top][0] if top50[cost][top][1] > 0.001 else '') for cost in costs])\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (python3_0901)",
   "language": "python",
   "name": "python3_0901"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-17T05:47:29.672988Z",
     "start_time": "2018-03-17T05:47:24.681392Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic tokenize\n",
    "- sent_tokenize\n",
    "- word_tokenize\n",
    "- wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-17T05:47:29.980696Z",
     "start_time": "2018-03-17T05:47:29.703967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello.', 'My name is yechan.', 'I like data science']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize\n",
    "\n",
    "sentences = 'Hello. My name is yechan. I like data science'\n",
    "result = sent_tokenize(sentences)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-17T05:47:30.013703Z",
     "start_time": "2018-03-17T05:47:30.010650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello My name is yechan I like data science']\n"
     ]
    }
   ],
   "source": [
    "sentences = 'Hello My name is yechan I like data science'\n",
    "result = sent_tokenize(sentences)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-17T05:47:30.096919Z",
     "start_time": "2018-03-17T05:47:30.026147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, My name is yechan.', 'How are you?', 'I like data science']\n"
     ]
    }
   ],
   "source": [
    "sentences = 'Hello, My name is yechan. How are you? I like data science'\n",
    "result = sent_tokenize(sentences)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-17T05:47:30.174564Z",
     "start_time": "2018-03-17T05:47:30.128392Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello',\n",
      " '.',\n",
      " 'My',\n",
      " 'name',\n",
      " 'is',\n",
      " 'yechan',\n",
      " '.',\n",
      " 'I',\n",
      " 'like',\n",
      " 'data',\n",
      " 'science']\n"
     ]
    }
   ],
   "source": [
    "sentences = 'Hello. My name is yechan. I like data science'\n",
    "result = word_tokenize(sentences)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-17T05:47:30.287763Z",
     "start_time": "2018-03-17T05:47:30.186758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of word_tokenize\n",
      "['All',\n",
      " 'she',\n",
      " 'talking',\n",
      " 'about',\n",
      " 'is',\n",
      " 'come',\n",
      " 'and',\n",
      " 'see',\n",
      " 'me',\n",
      " 'for',\n",
      " 'once',\n",
      " 'Come',\n",
      " 'and',\n",
      " 'see',\n",
      " 'em',\n",
      " 'for',\n",
      " 'once',\n",
      " 'You',\n",
      " 'do',\n",
      " \"n't\",\n",
      " 'ever',\n",
      " 'come',\n",
      " 'to',\n",
      " 'me',\n",
      " ',',\n",
      " 'you',\n",
      " 'do',\n",
      " \"n't\",\n",
      " 'ever',\n",
      " 'come',\n",
      " 'to',\n",
      " 'me',\n",
      " 'All',\n",
      " 'she',\n",
      " 'ever',\n",
      " 'say',\n",
      " 'is',\n",
      " 'come',\n",
      " 'and',\n",
      " 'see',\n",
      " 'me',\n",
      " 'for',\n",
      " 'once',\n",
      " 'Come',\n",
      " 'and',\n",
      " 'see',\n",
      " 'me',\n",
      " 'for',\n",
      " 'once',\n",
      " 'You',\n",
      " 'do',\n",
      " \"n't\",\n",
      " 'ever',\n",
      " 'come',\n",
      " 'to',\n",
      " 'me',\n",
      " ',',\n",
      " 'you',\n",
      " 'do',\n",
      " \"n't\",\n",
      " 'ever',\n",
      " 'come',\n",
      " 'to',\n",
      " 'me']\n",
      "\n",
      "\n",
      "Result of wordpunct_tokenize\n",
      "['All',\n",
      " 'she',\n",
      " 'talking',\n",
      " 'about',\n",
      " 'is',\n",
      " 'come',\n",
      " 'and',\n",
      " 'see',\n",
      " 'me',\n",
      " 'for',\n",
      " 'once',\n",
      " 'Come',\n",
      " 'and',\n",
      " 'see',\n",
      " 'em',\n",
      " 'for',\n",
      " 'once',\n",
      " 'You',\n",
      " 'don',\n",
      " \"'\",\n",
      " 't',\n",
      " 'ever',\n",
      " 'come',\n",
      " 'to',\n",
      " 'me',\n",
      " ',',\n",
      " 'you',\n",
      " 'don',\n",
      " \"'\",\n",
      " 't',\n",
      " 'ever',\n",
      " 'come',\n",
      " 'to',\n",
      " 'me',\n",
      " 'All',\n",
      " 'she',\n",
      " 'ever',\n",
      " 'say',\n",
      " 'is',\n",
      " 'come',\n",
      " 'and',\n",
      " 'see',\n",
      " 'me',\n",
      " 'for',\n",
      " 'once',\n",
      " 'Come',\n",
      " 'and',\n",
      " 'see',\n",
      " 'me',\n",
      " 'for',\n",
      " 'once',\n",
      " 'You',\n",
      " 'don',\n",
      " \"'\",\n",
      " 't',\n",
      " 'ever',\n",
      " 'come',\n",
      " 'to',\n",
      " 'me',\n",
      " ',',\n",
      " 'you',\n",
      " 'don',\n",
      " \"'\",\n",
      " 't',\n",
      " 'ever',\n",
      " 'come',\n",
      " 'to',\n",
      " 'me']\n"
     ]
    }
   ],
   "source": [
    "sentences = \"\"\"\n",
    "All she talking about is come and see me for once\n",
    "Come and see em for once\n",
    "You don't ever come to me, you don't ever come to me\n",
    "All she ever say is come and see me for once\n",
    "Come and see me for once\n",
    "You don't ever come to me, you don't ever come to me\n",
    "\"\"\"\n",
    "\n",
    "word_tokenize_result = word_tokenize(sentences)\n",
    "wordpunct_tokenized_result = wordpunct_tokenize(sentences)\n",
    "\n",
    "print(\"Result of word_tokenize\")\n",
    "pprint(word_tokenize_result)\n",
    "\n",
    "print('\\n')\n",
    "print(\"Result of wordpunct_tokenize\")\n",
    "pprint(wordpunct_tokenized_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Part-of-speech (POS) tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-17T05:47:30.688846Z",
     "start_time": "2018-03-17T05:47:30.310803Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('All', 'DT'),\n",
      " ('she', 'PRP'),\n",
      " ('talking', 'VBG'),\n",
      " ('bout', 'NN'),\n",
      " ('is', 'VBZ'),\n",
      " ('come', 'VBN'),\n",
      " ('and', 'CC'),\n",
      " ('see', 'VB'),\n",
      " ('me', 'PRP'),\n",
      " ('for', 'IN'),\n",
      " ('once', 'RB'),\n",
      " ('Come', 'NNP'),\n",
      " ('and', 'CC'),\n",
      " ('see', 'VB'),\n",
      " ('me', 'PRP'),\n",
      " ('for', 'IN'),\n",
      " ('once', 'RB'),\n",
      " ('You', 'PRP'),\n",
      " ('don', 'VBP'),\n",
      " (\"'\", \"''\"),\n",
      " ('t', 'JJ'),\n",
      " ('ever', 'RB'),\n",
      " ('come', 'VBP'),\n",
      " ('to', 'TO'),\n",
      " ('me', 'PRP'),\n",
      " (',', ','),\n",
      " ('you', 'PRP'),\n",
      " ('don', 'VBP'),\n",
      " (\"'\", \"''\"),\n",
      " ('t', 'JJ'),\n",
      " ('ever', 'RB'),\n",
      " ('come', 'VBP'),\n",
      " ('to', 'TO'),\n",
      " ('me', 'PRP'),\n",
      " ('All', 'PDT'),\n",
      " ('she', 'PRP'),\n",
      " ('ever', 'RB'),\n",
      " ('say', 'VBP'),\n",
      " ('is', 'VBZ'),\n",
      " ('come', 'JJ'),\n",
      " ('and', 'CC'),\n",
      " ('see', 'VB'),\n",
      " ('me', 'PRP'),\n",
      " ('for', 'IN'),\n",
      " ('once', 'RB'),\n",
      " ('Come', 'NNP'),\n",
      " ('and', 'CC'),\n",
      " ('see', 'VB'),\n",
      " ('me', 'PRP'),\n",
      " ('for', 'IN'),\n",
      " ('once', 'RB'),\n",
      " ('You', 'PRP'),\n",
      " ('don', 'VBP'),\n",
      " (\"'\", \"''\"),\n",
      " ('t', 'JJ'),\n",
      " ('ever', 'RB'),\n",
      " ('come', 'VBP'),\n",
      " ('to', 'TO'),\n",
      " ('me', 'PRP'),\n",
      " (',', ','),\n",
      " ('you', 'PRP'),\n",
      " ('don', 'VBP'),\n",
      " (\"'\", \"''\"),\n",
      " ('t', 'JJ'),\n",
      " ('ever', 'RB'),\n",
      " ('come', 'VBP'),\n",
      " ('to', 'TO'),\n",
      " ('me', 'PRP')]\n"
     ]
    }
   ],
   "source": [
    "sentences = \"\"\"\n",
    "All she talking bout is come and see me for once\n",
    "Come and see me for once\n",
    "You don't ever come to me, you don't ever come to me\n",
    "All she ever say is come and see me for once\n",
    "Come and see me for once\n",
    "You don't ever come to me, you don't ever come to me\n",
    "\"\"\"\n",
    "\n",
    "pos_result = nltk.pos_tag(nltk.tokenize.wordpunct_tokenize(sentences))\n",
    "pprint(pos_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalize\n",
    "- stemming\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-17T05:47:30.721955Z",
     "start_time": "2018-03-17T05:47:30.707278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the women run in the fog pass bunni work as comput scientist\n",
      "['the', 'women', 'run', 'in', 'the', 'fog', 'pass', 'bunni', 'work', 'as', 'comput', 'scientist']\n",
      "the wom run in the fog pass bunny work as comput sci\n",
      "['the', 'wom', 'run', 'in', 'the', 'fog', 'pass', 'bunny', 'work', 'as', 'comput', 'sci']\n",
      "the women run in the fog pass bunni work as comput scientist\n",
      "['the', 'women', 'run', 'in', 'the', 'fog', 'pass', 'bunni', 'work', 'as', 'comput', 'scientist']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "text = list(nltk.word_tokenize('The women running in the fog passed bunnies working as computer scientists'))\n",
    "\n",
    "snowball = SnowballStemmer('english')\n",
    "lancaster = LancasterStemmer()\n",
    "porter = PorterStemmer()\n",
    "\n",
    "for stemmer in [snowball, lancaster, porter]:\n",
    "    stemmed_text = [stemmer.stem(word) for word in text]\n",
    "    print(\" \".join(stemmed_text))\n",
    "    print(stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-17T05:47:34.032022Z",
     "start_time": "2018-03-17T05:47:32.422291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The woman running in the fog passed bunny working a computer scientist\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# The women running in the fog passed bunnies working as computer scientists\n",
    "\n",
    "# use part of speech tag, we'll see this in machine learning\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(word) for word in text]\n",
    "print(\" \".join(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-17T05:50:02.797938Z",
     "start_time": "2018-03-17T05:50:02.734708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eagle', 'fly', 'midnight']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "## Module constant\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# stopwords메서드로 불용어처리를 설정함\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation = string.punctuation\n",
    "\n",
    "def tagwn(tag):\n",
    "    return {\n",
    "        'N': wn.NOUN,\n",
    "        'V': wn.VERB,\n",
    "        'R': wn.ADV,\n",
    "        'J': wn.ADJ\n",
    "    }.get(tag[0], wn.NOUN)\n",
    "\n",
    "\n",
    "def normalize(text):\n",
    "    for token, tag in nltk.pos_tag(nltk.wordpunct_tokenize(text)):\n",
    "        token = token.lower()\n",
    "        if token in stopwords or token in punctuation:\n",
    "            continue\n",
    "        token = lemmatizer.lemmatize(token, tagwn(tag))\n",
    "        yield token\n",
    "        \n",
    "print(list(normalize('The eagle ! up a flies at midnight')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-17T05:50:05.690073Z",
     "start_time": "2018-03-17T05:50:05.681633Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-17T05:50:05.951002Z",
     "start_time": "2018-03-17T05:50:05.943513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'of', 'our', 'she', 'o', 'haven', 'that', 'over', 'nor', 'to', 'myself', 'we', 'as', 'all', 'against', 'for', 'some', 'won', 'did', 'do', 'it', 'few', 'are', 'where', 'the', 'themselves', 'out', 'during', 'down', 'your', 'at', 'hers', 'in', 'other', 'about', 'having', 'its', 'yours', 'my', 'with', 'too', 'not', 'has', 'each', 'until', 'below', 'there', 'didn', 'while', 'am', 'was', 'before', 'only', 'because', 'd', 'on', 'have', 'then', 'theirs', 'mightn', 'yourself', 'he', 'be', 'doesn', 'shan', 'from', 'which', 'same', 'himself', 'had', 'their', 'any', 'such', 'a', 'after', 'but', 'mustn', 'can', 'off', 'once', 'don', 'they', 'hasn', 'will', 'm', 'hadn', 'y', 'just', 'i', 're', 'this', 'herself', 'should', 'weren', 'isn', 'an', 'ours', 'further', 'shouldn', 'if', 'these', 'into', 'aren', 'up', 'll', 'again', 'here', 's', 'why', 'through', 'no', 'them', 'how', 'her', 'above', 'couldn', 'so', 'needn', 'wasn', 'is', 'were', 'his', 'most', 'own', 'wouldn', 'under', 'been', 'who', 'more', 'ma', 'when', 't', 'ourselves', 'by', 'ain', 'those', 'and', 'itself', 'or', 'whom', 'him', 'yourselves', 'very', 'both', 'being', 'me', 've', 'does', 'than', 'you', 'between', 'now', 'what', 'doing'}\n"
     ]
    }
   ],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Named-entity recognition(NER)\n",
    "- Maximum entropy based NER\n",
    "- Standford NER packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-17T05:50:17.606718Z",
     "start_time": "2018-03-17T05:50:17.261677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  LG/NNP\n",
      "  electronics/NNS\n",
      "  released/VBD\n",
      "  the/DT\n",
      "  smart/JJ\n",
      "  phone/NN\n",
      "  '/''\n",
      "  G6/NNP\n",
      "  '/POS\n",
      "  in/IN\n",
      "  (GPE April/NNP)\n",
      "  ,/,\n",
      "  2017/CD\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "text = \"LG electronics released the smart phone 'G6' in April, 2017.\"\n",
    "print(nltk.ne_chunk(nltk.pos_tag(nltk.wordpunct_tokenize(text))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T06:12:28.345898Z",
     "start_time": "2018-03-16T06:12:28.010862Z"
    }
   },
   "source": [
    "\n",
    "```python\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "stanford_data = 'standford-ner-2016-10-31/classifier/english.all.3class.distsim.crf.sr.gz'\n",
    "stanford_jar = 'standford-ner-2016-10-31/stanford-ner-3.7.0.jar'\n",
    "\n",
    "text = \"Samsung electronics Microsoft research GE LG Baidu Amazon\"\n",
    "st = StanfordNERTagger(stanford_data, stanford_jar, 'utf-8')\n",
    "for i in st.tag(text.split()):\n",
    "    print('[' + i[1] + ']' + i[0])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parsing\n",
    "- Parsing using a grammar\n",
    "- StandfordParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-17T05:50:27.070826Z",
     "start_time": "2018-03-17T05:50:27.067504Z"
    }
   },
   "outputs": [],
   "source": [
    "grammar = nltk.grammar.CFG.fromstring(\n",
    "\"\"\"\n",
    "    S -> NP PUNCT | NP\n",
    "    NP -> N N | ADJP NP | DET N | DET ADJP\n",
    "    ADJP -> ADJ NP | ADJ N\n",
    "    \n",
    "    DET -> 'an' | 'the' | 'a' | 'that'\n",
    "    N -> 'airplane' | 'runaway' | 'lawn' | 'chair' | 'person'\n",
    "    ADJ -> 'red' | 'slow' | 'tired' | 'long'\n",
    "    PUNCT -> '.'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-17T05:50:28.764231Z",
     "start_time": "2018-03-17T05:50:28.757558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (DET the) (ADJP (ADJ long) (N runaway))))\n"
     ]
    }
   ],
   "source": [
    "# 포함된 단어를 파싱\n",
    "def parse(sent):\n",
    "    sent = sent.lower()\n",
    "    parser = nltk.parse.ChartParser(grammar)\n",
    "    for p in parser.parse(nltk.word_tokenize(sent)):\n",
    "        yield p\n",
    "        \n",
    "for tree in parse(\"the long runaway\"):\n",
    "    tree.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "\n",
    "stanford_model = 'stanford-parser-full-2016-10-31/stanford-parser-3.7.0-models.jar'\n",
    "stanford_jar = 'stanford-parser-full-2016-10-31/stanford-parser.jar'\n",
    "\n",
    "st = StanfordParser(stanford_model, stanford_jar)\n",
    "sent = \"The man hit the building with the baseball bat.\"\n",
    "for tree in st.parse(nltk.wordpunct_tokenize(sent)):\n",
    "    tree.pprint()\n",
    "#     tree.draw()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (python3_0901)",
   "language": "python",
   "name": "python3_0901"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
